{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import tiktoken\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPEN_AI_KEY\"] = api_key\n",
    "print(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "# Function to calculate token usage\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bhull\\anaconda3\\envs\\langchain_env\\lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed. Results saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\evaluation_results.json\n"
     ]
    }
   ],
   "source": [
    "### IN CONTEXT LEARNING EVAL\n",
    "\n",
    "import os\n",
    "import ast\n",
    "from difflib import SequenceMatcher\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Function to check syntax correctness\n",
    "def check_syntax_correctness(code):\n",
    "    try:\n",
    "        ast.parse(code)  # Parse the code to check syntax\n",
    "        return True, None\n",
    "    except SyntaxError as e:\n",
    "        return False, str(e)\n",
    "\n",
    "# Function to calculate structural similarity using AST\n",
    "def calculate_ast_similarity(expected_code, generated_code):\n",
    "    try:\n",
    "        expected_ast = ast.dump(ast.parse(expected_code), annotate_fields=False)\n",
    "        generated_ast = ast.dump(ast.parse(generated_code), annotate_fields=False)\n",
    "        similarity = SequenceMatcher(None, expected_ast, generated_ast).ratio()\n",
    "        return similarity\n",
    "    except Exception as e:\n",
    "        return 0.0  # Return 0 if there's an error parsing AST\n",
    "\n",
    "# Function to calculate textual similarity\n",
    "def calculate_textual_similarity(expected_code, generated_code):\n",
    "    # Tokenize the code lines for BLEU and Jaccard calculation\n",
    "    expected_lines = expected_code.splitlines()\n",
    "    generated_lines = generated_code.splitlines()\n",
    "\n",
    "    # BLEU Score (unigram)\n",
    "    bleu_score = sentence_bleu([expected_lines], generated_lines, weights=(1, 0, 0, 0))\n",
    "\n",
    "    # Jaccard Index on line-level tokens\n",
    "    expected_tokens = set(\" \".join(expected_lines).split())\n",
    "    generated_tokens = set(\" \".join(generated_lines).split())\n",
    "    jaccard_index = len(expected_tokens & generated_tokens) / len(expected_tokens | generated_tokens)\n",
    "\n",
    "    return bleu_score, jaccard_index\n",
    "\n",
    "# Evaluation loop\n",
    "def evaluate_generated_code(generated_folder, expected_folder, output_file=\"evaluation_results.json\"):\n",
    "    evaluation_results = []\n",
    "\n",
    "    # Get all generated files\n",
    "    generated_files = [f for f in os.listdir(generated_folder) if f.endswith('_generated_ICL.py')]\n",
    "\n",
    "    for gen_file in generated_files:\n",
    "        base_name = gen_file.replace('_generated_ICL.py', '').replace(\"prompt_\", \"\")\n",
    "\n",
    "        # Corresponding expected file in the expected folder\n",
    "        expected_file = os.path.join(expected_folder, f\"modified_{base_name}.py\")\n",
    "        generated_file = os.path.join(generated_folder, gen_file)\n",
    "\n",
    "        # Skip if expected file doesn't exist\n",
    "        if not os.path.exists(expected_file):\n",
    "            print(f\"Expected file not found for {gen_file}\")\n",
    "            continue\n",
    "\n",
    "        # Load expected and generated code\n",
    "        with open(expected_file, 'r') as ef:\n",
    "            expected_code = ef.read()\n",
    "        with open(generated_file, 'r') as gf:\n",
    "            generated_code = gf.read()\n",
    "\n",
    "        # Syntax correctness check\n",
    "        syntax_correct, syntax_error = check_syntax_correctness(generated_code)\n",
    "\n",
    "        # Structural similarity using AST\n",
    "        ast_similarity = calculate_ast_similarity(expected_code, generated_code)\n",
    "\n",
    "        # Textual similarity\n",
    "        bleu_score, jaccard_index = calculate_textual_similarity(expected_code, generated_code)\n",
    "\n",
    "        # Collect results\n",
    "        evaluation_results.append({\n",
    "            \"file\": gen_file,\n",
    "            \"syntax_correct\": syntax_correct,\n",
    "            \"syntax_error\": syntax_error if not syntax_correct else None,\n",
    "            \"ast_similarity\": ast_similarity,\n",
    "            \"bleu_score\": bleu_score,\n",
    "            \"jaccard_index\": jaccard_index\n",
    "        })\n",
    "\n",
    "    # Save results to a file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        import json\n",
    "        json.dump(evaluation_results, f, indent=4)\n",
    "\n",
    "    print(f\"Evaluation completed. Results saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "generated_folder = r\"C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\generated_inner_analyzers_ICL\"\n",
    "expected_folder = r\"C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\generated_inner_working_dataset\"\n",
    "output_file = r\"C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\evaluation_results.json\"\n",
    "\n",
    "evaluate_generated_code(generated_folder, expected_folder, output_file=output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "inner_dataset_baseline = EvaluationDataset()\n",
    "\n",
    "# Add as test cases\n",
    "inner_dataset_baseline.add_test_cases_from_csv_file(\n",
    "    # file_path is the absolute path to you .csv file\n",
    "    file_path=r\"C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\inner_dataset2.csv\",\n",
    "    input_col_name=\"input\",\n",
    "    actual_output_col_name=\"actual_output\",\n",
    "    expected_output_col_name=\"expected_output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Hallucination Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mHallucination Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) sequentially: |██████████|100% (1/1) [Time Taken: 00:37, 37.59s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because there are no factual alignments or contradictions, indicating that the actual output is perfectly aligned with the contexts provided., error: None)\n",
      "  - ✅ Answer Relevancy (score: 0.7096774193548387, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.71 because the output contains relevant code for the KPIManagerModified class with appropriate methods for KPI management. However, the presence of multiple irrelevant comments, author placeholders, and other non-functional elements slightly detracts from the focus on the code's functionality, which prevents the score from being higher., error: None)\n",
      "  - ✅ Correctness (GEval) (score: 0.6607769200686414, threshold: 0.65, strict: False, evaluation model: gpt-4o, reason: The actual output aligns well with the expected output in terms of class structure and functionality, including methods for enabling KPIs and querying them. However, it deviates by using importlib and logging instead of inspect and os, and it lacks some expected parameters in methods like 'remote_query_kpi'. It also does not include 'cell' as a parameter in 'enable_kpi'., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: \n",
      "        You are an AI assistant that generates code for inner analyzers using the Mobileinsight-core Python library, a library that enables below-IP,         fine-grained mobile network analytics on end devices. It is a cross-platform package for mobile network monitoring and analysis.\n",
      "\n",
      "        For context, I will be giving a few examples of a prompt + outer analyzer code pairs along with their corresponding expected inner analyzer code.\n",
      "\n",
      "        Then I will give the main target prompt that you need to follow in order to generate an inner analyzer.\n",
      "\n",
      "        NOTE: PLEASE PROVIDE ONLY THE CODE AND NOTHING ELSE, AS THIS OUTPUT IS BEING DIRECTLY SAVED TO A .PY FILE AND RAN AUTONOMOUSLY.         ADDITIONALLY, ENSURE THAT YOU PROVIDE THE FULL COMPLETE CODE, AND DO NOT LEAVE OUT ANY PARTS FOR THE USER TO COMPLETE. THE CODE SHOULD FULLY RUN         WITH NO ADDITIONAL MODIFICATIONS REQUIRED.\n",
      "        Example 1:\n",
      "Prompt: I want you to define a class `ModemDebugAnalyzerModified` that inherits from a base `Analyzer` class, and processes modem debug messages to extract specific metrics:\n",
      "\n",
      "1. Class Definition: `ModemDebugAnalyzerModified`\n",
      "This class extends from the base `Analyzer` class. It configures the source by enabling logs for \"Modem_debug_message\". It processes these messages through the `__msg_callback` function, which decodes incoming messages and performs additional analyses:\n",
      "   - Logs the original modem debug message.\n",
      "   - Computes and logs the word count of the message.\n",
      "   - Checks for the presence of the keyword 'Error' within the message and logs its detection.\n",
      "\n",
      "2. Integration with Outer Analyzer: \n",
      "The class will be integrated into an outer analyzer script, which utilizes the `ModemDebugAnalyzerModified` class to evaluate metrics from the replayed logs. \n",
      "\n",
      "3. Execution Logic:\n",
      "The outer analyzer will set the input path for the log files, initialize the `ModemDebugAnalyzerModified` class, and configure it with an `OfflineReplayer` as the data source. The analysis is executed by replaying the logs, processing each message to extract and log the specified metrics, and saving the results to a specified output file. The execution should be robust, handling any potential exceptions during log replay and analysis.\n",
      "#!/usr/bin/python\n",
      "# Filename: offline-analysis-example.py\n",
      "import os\n",
      "import sys\n",
      "\n",
      "\"\"\"\n",
      "Offline analysis by replaying logs\n",
      "\"\"\"\n",
      "\n",
      "# Import MobileInsight modules\n",
      "from mobile_insight.monitor import OfflineReplayer\n",
      "from mobile_insight.analyzer import MsgLogger, ModemDebugAnalyzer\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # Initialize a monitor\n",
      "    src = OfflineReplayer()\n",
      "    src.set_input_path(\"./logs/\")\n",
      "    # src.enable_log_all()\n",
      "\n",
      "    src.enable_log(\"LTE_PHY_Serv_Cell_Measurement\")\n",
      "    src.enable_log(\"5G_NR_RRC_OTA_Packet\")\n",
      "    src.enable_log(\"LTE_RRC_OTA_Packet\")\n",
      "    src.enable_log(\"LTE_NB1_ML1_GM_DCI_Info\")\n",
      "\n",
      "    logger = MsgLogger()\n",
      "    logger.set_decode_format(MsgLogger.XML)\n",
      "    logger.set_dump_type(MsgLogger.FILE_ONLY)\n",
      "    logger.save_decoded_msg_as(\"./test.txt\")\n",
      "    logger.set_source(src)\n",
      "\n",
      "    modem_debug_analyzer = ModemDebugAnalyzer()\n",
      "    modem_debug_analyzer.set_source(src)\n",
      "\n",
      "    # Start the monitoring\n",
      "    src.run()\n",
      "\n",
      " Expected Output:\n",
      "#!/usr/bin/python\n",
      "# Filename: modem_debug_analyzer_modified.py\n",
      "\"\"\"\n",
      "A modified debugger for cellular interface with additional metrics\n",
      "\n",
      "Author: Yuanjie Li (Modified)\n",
      "\"\"\"\n",
      "\n",
      "from mobile_insight.analyzer.analyzer import *\n",
      "\n",
      "__all__ = [\"ModemDebugAnalyzerModified\"]\n",
      "\n",
      "class ModemDebugAnalyzerModified(Analyzer):\n",
      "\n",
      "    def __init__(self):\n",
      "        Analyzer.__init__(self)\n",
      "\n",
      "        self.add_source_callback(self.__msg_callback)\n",
      "\n",
      "    def set_source(self, source):\n",
      "        \"\"\"\n",
      "        Set the trace source. Enable the cellular signaling messages\n",
      "\n",
      "        :param source: the trace source (collector).\n",
      "        \"\"\"\n",
      "        Analyzer.set_source(self, source)\n",
      "\n",
      "        # Phy-layer logs\n",
      "        source.enable_log(\"Modem_debug_message\")\n",
      "\n",
      "    def __msg_callback(self, msg):\n",
      "\n",
      "        if msg.type_id == \"Modem_debug_message\":\n",
      "\n",
      "            log_item = msg.data.decode()\n",
      "\n",
      "            if 'Msg' in log_item:\n",
      "                # Log the original message\n",
      "                self.log_info(log_item[\"Msg\"])\n",
      "\n",
      "                # Additional metric: count the number of words in the message\n",
      "                word_count = len(log_item[\"Msg\"].split())\n",
      "                self.log_info(f\"Word count in message: {word_count}\")\n",
      "\n",
      "                # Additional metric: check if 'Error' keyword is in the message\n",
      "                if 'Error' in log_item[\"Msg\"]:\n",
      "                    self.log_info(\"Error keyword detected in message.\")\n",
      "\n",
      "Example 2:\n",
      "Prompt: I want you to define a class `MsgStatisticsModified` that inherits from a base `Analyzer` class, and returns statistics for cellular messages, including message type counts, arrival intervals, and average message lengths:\n",
      "\n",
      "1. Class Definition: `MsgStatisticsModified`\n",
      "This class extends from a base `Analyzer` class. It should initialize and maintain dictionaries to store message type statistics, arrival intervals, lengths, and average lengths. The `set_source` method sets the trace source and enables all cellular signaling messages.\n",
      "\n",
      "2. Message Processing: `__msg_callback`\n",
      "The `__msg_callback` function processes each message to update the statistics:\n",
      "   - For each message, update the count of the message type.\n",
      "   - Record the timestamp for arrival intervals.\n",
      "   - Capture the message length from fields like `log_msg_len`, `Msg Length`, or `Message Length`.\n",
      "   - Calculate the average message length for each message type.\n",
      "\n",
      "3. Reset Functionality: `reset`\n",
      "Include a `reset` method to clear all statistics, allowing the analyzer to be reused for different analysis sessions.\n",
      "\n",
      "This class will be used by the outer analyzer file to evaluate metrics such as message type statistics, arrival intervals, and average message lengths from offline log data.\n",
      "#!/usr/bin/python\n",
      "# Filename: msg-statistics-example.py\n",
      "import os\n",
      "import sys\n",
      "\n",
      "# Import MobileInsight modules\n",
      "from mobile_insight.monitor import OfflineReplayer\n",
      "from mobile_insight.analyzer.msg_statistics import MsgStatistics\n",
      "\n",
      "\"\"\"\n",
      "This example shows how to get basic statistics of a offline log\n",
      "\"\"\"\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # Initialize a 3G/4G monitor\n",
      "    src = OfflineReplayer()\n",
      "    src.set_input_path(\"./offline_log_example.mi2log\")\n",
      "\n",
      "    statistics = MsgStatistics()\n",
      "    statistics.set_source(src)\n",
      "\n",
      "    # Start the monitoring\n",
      "    src.run()\n",
      "\n",
      "    # Save results\n",
      "    f_statistics = open('./msg_type_statistics.txt', 'w')\n",
      "    for item in statistics.msg_type_statistics:\n",
      "        f_statistics.write(\n",
      "            item + \" \" + str(statistics.msg_type_statistics[item]) + \"\\n\")\n",
      "    f_statistics.close()\n",
      "\n",
      "    f_rate = open('./msg_arrival_rate.txt', 'w')\n",
      "    for item in statistics.msg_arrival_rate:\n",
      "        f_rate.write(item + \" \")\n",
      "        for k in range(1, len(statistics.msg_arrival_rate[item])):\n",
      "            f_rate.write(str(\n",
      "                (statistics.msg_arrival_rate[item][k] - statistics.msg_arrival_rate[item][k - 1]).total_seconds() * 1000) + \" \")\n",
      "        f_rate.write(\"\\n\")\n",
      "    f_rate.close()\n",
      "\n",
      "    f_msg_len = open('./msg_length.txt', 'w')\n",
      "    for item in statistics.msg_lengh:\n",
      "        f_msg_len.write(item + \" \")\n",
      "        for k in range(0, len(statistics.msg_lengh[item])):\n",
      "            f_msg_len.write(str(statistics.msg_lengh[item][k]) + \" \")\n",
      "        f_msg_len.write(\"\\n\")\n",
      "    f_msg_len.close()\n",
      "\n",
      " Expected Output:\n",
      "#!/usr/bin/python\n",
      "# Filename: msg_statistics_modified.py\n",
      "\"\"\"\n",
      "A modified analyzer to study the cellular message statistics, arrival interval time,\n",
      "and calculate the average message length\n",
      "\n",
      "Author: Yuanjie Li\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "from mobile_insight.analyzer.analyzer import *\n",
      "\n",
      "__all__ = [\"MsgStatisticsModified\"]\n",
      "\n",
      "\n",
      "class MsgStatisticsModified(Analyzer):\n",
      "\n",
      "    def __init__(self):\n",
      "        Analyzer.__init__(self)\n",
      "\n",
      "        self.add_source_callback(self.__msg_callback)\n",
      "\n",
      "        self.msg_type_statistics = {}  # type_id->msg_count\n",
      "\n",
      "        self.msg_arrival_rate = {}  # type_id->list of arrival interval\n",
      "\n",
      "        self.msg_lengh = {}  # type_id->list of message length\n",
      "\n",
      "        self.msg_avg_length = {}  # type_id->average message length\n",
      "\n",
      "    def reset(self):\n",
      "        self.msg_type_statistics = {}  # type_id->msg_count\n",
      "\n",
      "        self.msg_arrival_rate = {}  # type_id->list of arrival interval\n",
      "\n",
      "        self.msg_lengh = {}  # type_id->list of message length\n",
      "\n",
      "        self.msg_avg_length = {}  # type_id->average message length\n",
      "\n",
      "    def set_source(self, source):\n",
      "        \"\"\"\n",
      "        Set the trace source. Enable the cellular signaling messages\n",
      "\n",
      "        :param source: the trace source (collector).\n",
      "        \"\"\"\n",
      "        Analyzer.set_source(self, source)\n",
      "        source.enable_log_all()\n",
      "\n",
      "    def __msg_callback(self, msg):\n",
      "\n",
      "        log_item = msg.data.decode()\n",
      "\n",
      "        if msg.type_id not in self.msg_type_statistics:\n",
      "            self.msg_type_statistics[msg.type_id] = 1\n",
      "        else:\n",
      "            self.msg_type_statistics[msg.type_id] = self.msg_type_statistics[msg.type_id] + 1\n",
      "\n",
      "        if msg.type_id not in self.msg_arrival_rate:\n",
      "            self.msg_arrival_rate[msg.type_id] = [log_item[\"timestamp\"]]\n",
      "        else:\n",
      "            self.msg_arrival_rate[msg.type_id].append(log_item[\"timestamp\"])\n",
      "\n",
      "        if msg.type_id not in self.msg_lengh:\n",
      "            if \"log_msg_len\" in log_item:\n",
      "                self.msg_lengh[msg.type_id] = [log_item[\"log_msg_len\"]]\n",
      "            elif \"Msg Length\" in log_item:\n",
      "                self.msg_lengh[msg.type_id] = [log_item[\"Msg Length\"]]\n",
      "            elif \"Message Length\" in log_item:\n",
      "                self.msg_lengh[msg.type_id] = [log_item[\"Message Length\"]]\n",
      "        else:\n",
      "            if \"log_msg_len\" in log_item:\n",
      "                self.msg_lengh[msg.type_id].append(log_item[\"log_msg_len\"])\n",
      "            elif \"Msg Length\" in log_item:\n",
      "                self.msg_lengh[msg.type_id].append(log_item[\"Msg Length\"])\n",
      "            elif \"Message Length\" in log_item:\n",
      "                self.msg_lengh[msg.type_id].append(log_item[\"Message Length\"])\n",
      "\n",
      "        # Calculate average message length\n",
      "        if msg.type_id in self.msg_lengh:\n",
      "            total_length = sum(self.msg_lengh[msg.type_id])\n",
      "            count = len(self.msg_lengh[msg.type_id])\n",
      "            self.msg_avg_length[msg.type_id] = total_length / count if count > 0 else 0\n",
      "\n",
      "Example 3:\n",
      "Prompt: I want you to define a class `KPIManagerModified` that inherits from a base `Analyzer` class, providing modified calculations and logging for KPIs:\n",
      "\n",
      "1. Class Definition: `KPIManagerModified`\n",
      "This class extends the `Analyzer` class to offer a unified interface for tracking and querying KPIs. It includes a mechanism to identify and load supported KPIs from the `mobile_insight` library.\n",
      "\n",
      "   - Initialization: The constructor initializes the base `Analyzer` class and calls a helper function `__check_kpis` which dynamically identifies supported KPI analyzers by inspecting the `mobile_insight.analyzer.kpi` module. It logs the available KPIs.\n",
      "\n",
      "   - KPI Listing: The `list_kpis` function returns a list of all available KPI names that can be monitored.\n",
      "\n",
      "   - KPI Enabling: The `enable_kpi` function allows for enabling a specific KPI by its name. It includes modifications such as logging additional information when a KPI is activated. It also allows setting a periodicity and whether to enable local storage.\n",
      "\n",
      "   - Enable All: The `enable_all_kpis` method enables monitoring for all identified KPIs.\n",
      "\n",
      "2. KPI Query Functions:\n",
      "   - Local Query: `local_query_kpi` allows querying the locally observed KPI values. It includes modified logic to adjust query behavior based on a given mode (e.g., 'cell') and logs additional information based on the query mode.\n",
      "\n",
      "   - Remote Query: `remote_query_kpi` facilitates querying KPI data from a remote cloud service, providing enhanced logging to track remote query operations.\n",
      "\n",
      "3. Functionality and Usage:\n",
      "The class offers enhanced logging and additional functionality over a traditional KPI manager, making it suitable for scenarios requiring detailed tracking and querying of KPI metrics with modified calculations and handling logic.\n",
      "# Usage: python kpi=manager-test.py [dirname]\n",
      "# Example1: python kpi-manager-test-experimental.py logs/bler_sample.mi2log \n",
      "# (For testing KPI BLER)\n",
      "# Example2: python kpi-manager-test-experimental.py logs/data_sample.mi2log \n",
      "# (For testing KPI DL_PDCP_LOSS, HANDOVER_PREDICTION, HANDOVER_LATENCY, HANDOVER_HOL)\n",
      "# import os\n",
      "import sys\n",
      "\n",
      "from mobile_insight.monitor import OfflineReplayer\n",
      "from mobile_insight.analyzer.kpi import KPIManager, KpiAnalyzer\n",
      "import cProfile\n",
      "\n",
      "\n",
      "def kpi_manager_example():\n",
      "\n",
      "    src = OfflineReplayer()\n",
      "    src.set_input_path('./logs/offline_log_examples/20201115_181637_Xiaomi-Mi10_46000.mi2log')\n",
      "\n",
      "    kpi_manager = KPIManager()\n",
      "    # print \"All supported KPIs:\", str(kpi_manager.list_kpis())\n",
      "\n",
      "    # Test experimental KPIs - data plane\n",
      "    kpi_manager.enable_kpi(\"KPI.Wireless.BLER\") # test log: bler_sample\n",
      "    kpi_manager.enable_kpi(\"KPI.Wireless.DL_PDCP_LOSS\") # test log: data_sample\n",
      "    kpi_manager.enable_kpi(\"KPI.Wireless.UL_PDCP_LOSS\")\n",
      "\n",
      "    # Test experimental KPIs - handover\n",
      "    kpi_manager.enable_kpi(\"KPI.Mobility.HANDOVER_PREDICTION\") # test log: data_sample\n",
      "    kpi_manager.enable_kpi(\"KPI.Mobility.HANDOVER_LATENCY\") # test log: data_sample\n",
      "    kpi_manager.enable_kpi(\"KPI.Mobility.HANDOVER_HOL\") # test log: data_sample\n",
      "\n",
      "    kpi_manager.set_source(src)\n",
      "\n",
      "    src.run()\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    kpi_manager_example()\n",
      "\n",
      " Expected Output:\n",
      "#!/usr/bin/python\n",
      "# Filename: kpi_manager_modified.py\n",
      "\"\"\"\n",
      "kpi_manager_modified.py\n",
      "An unified interface for users to track and query KPIs with modified calculations\n",
      "\n",
      "Author: Yuanjie Li\n",
      "\"\"\"\n",
      "\n",
      "__all__ = [\"KPIManagerModified\"]\n",
      "\n",
      "from ..analyzer import *\n",
      "import sys, inspect, os\n",
      "\n",
      "\n",
      "class KPIManagerModified(Analyzer):\n",
      "\n",
      "    \"\"\"\n",
      "    An unified interface for users to track and query KPIs\n",
      "    \"\"\"\n",
      "\n",
      "    supported_kpis={} # Supported KPIs: kpi_name -> KPIAnalyzer name\n",
      "\n",
      "    def __init__(self):\n",
      "        Analyzer.__init__(self)\n",
      "        self.__check_kpis()\n",
      "\n",
      "\n",
      "\n",
      "    def __check_kpis(self):\n",
      "\n",
      "        \"\"\"\n",
      "        Find and include all supported KPIs into KPIManager.supported_kpis\n",
      "        \"\"\"\n",
      "        module_tmp = __import__(\"mobile_insight\")\n",
      "        for item in inspect.getmembers(module_tmp.analyzer.kpi, inspect.isclass):\n",
      "            if item[1].__bases__[0].__name__ ==  \"KpiAnalyzer\":\n",
      "                tmp_module = item[1]()\n",
      "                for kpi in tmp_module.list_kpis():\n",
      "                        KPIManagerModified.supported_kpis[kpi] = item[0]\n",
      "                        self.log_info(kpi)\n",
      "        \n",
      "\n",
      "    def list_kpis(self):\n",
      "        \"\"\"\n",
      "        Return a list of available KPIs \n",
      "\n",
      "        :returns: a list of string, each of which is a KPI name\n",
      "        \"\"\"\n",
      "        return list(self.supported_kpis.keys())\n",
      "\n",
      "    def enable_all_kpis(self, enable_storage = False):\n",
      "        \"\"\"\n",
      "        Enable all KPIs' monitoring\n",
      "        \n",
      "        :param enable_storage: Whether to locally store the kpi. False by default\n",
      "        :type enable_storage: boolean\n",
      "        \"\"\"\n",
      "        for kpi_name in self.list_kpis():\n",
      "            self.enable_kpi(kpi_name, enable_storage)\n",
      "\n",
      "\n",
      "    def enable_kpi(self, kpi_name, periodicity='0s', cell=None, enable_storage = True):\n",
      "        \"\"\"\n",
      "        Enable the KPI monitoring with slight modification\n",
      "\n",
      "        :param kpi_name: The KPI to be monitored\n",
      "        :type kpi_name: string\n",
      "        :param enable_storage: Whether to locally store the kpi. False by default\n",
      "        :type enable_storage: boolean\n",
      "        :returns: True if successfully activated, False otherwise\n",
      "        \"\"\"\n",
      "\n",
      "        if kpi_name not in self.supported_kpis:\n",
      "            self.log_warning(\"KPI does not exist: \"+kpi_name)\n",
      "            return False\n",
      "\n",
      "        try: \n",
      "            kpi_analyzer_name = self.supported_kpis[kpi_name]\n",
      "            self.include_analyzer(kpi_analyzer_name, [])\n",
      "            self.get_analyzer(kpi_analyzer_name).enable_local_storage(enable_storage)\n",
      "            self.get_analyzer(kpi_analyzer_name).set_periodicity(kpi_name, periodicity)\n",
      "            self.get_analyzer(kpi_analyzer_name).set_cell(kpi_name, cell)\n",
      "            # Modification: Log additional info for KPI activation\n",
      "            self.log_info(f\"Enable KPI: {kpi_name} with periodicity: {periodicity} and storage: {enable_storage}\")\n",
      "            return True\n",
      "        except Exception as e:\n",
      "            # Import failure\n",
      "            self.log_warning(\"Fail to activate KPI: \"+kpi_name)    \n",
      "            return False\n",
      "\n",
      "\n",
      "    def local_query_kpi(self, kpi_name, mode = 'cell', timestamp = None):\n",
      "        \"\"\"\n",
      "        Query the phone's locally observed KPI\n",
      "\n",
      "        :param kpi_name: The KPI to be queried\n",
      "        :type kpi_name: string\n",
      "        :param timestamp: The timestamp of the KPI. If None, this function returns the latest KPI\n",
      "        :type timestamp: datetime\n",
      "        :returns: The KPI value, or None if the KPI is not available\n",
      "        \"\"\"\n",
      "        if kpi_name not in self.supported_kpis:\n",
      "            self.log_warning(\"KPI does not exist: \"+kpi_name)\n",
      "            return None\n",
      "\n",
      "        kpi_agent = self.get_analyzer(self.supported_kpis[kpi_name])\n",
      "        if not kpi_agent:\n",
      "            # KPI analyzer not triggered\n",
      "            self.log_warning(\"KPI not activated yet: \"+kpi_name)\n",
      "            self.enable_kpi(kpi_name)\n",
      "            return None\n",
      "\n",
      "        # Modification: Adjust query mode logic (e.g., simulate different processing)\n",
      "        if mode == 'cell':\n",
      "            self.log_info(f\"Querying KPI: {kpi_name} in cell mode\")\n",
      "        else:\n",
      "            self.log_info(f\"Querying KPI: {kpi_name} in {mode} mode\")\n",
      "        \n",
      "        return kpi_agent.local_query_kpi(kpi_name, mode, timestamp)\n",
      "\n",
      "    def remote_query_kpi(self, kpi_name, phone_model, operator, gps, timestamp):\n",
      "        \"\"\"\n",
      "        Query the remote cloud for the KPI\n",
      "\n",
      "        :param kpi_name: The KPI to be queried\n",
      "        :type kpi_name: string\n",
      "        :param phone_model: The the phone model\n",
      "        :type phone_model: string\n",
      "        :param operator: The network operator\n",
      "        :type operator: string\n",
      "        :param gps: The GPS coordinate\n",
      "        :type gps: string\n",
      "        :param timestamp: The timestamp of the KPI. \n",
      "        :type timestamp: datetime\n",
      "        :returns: The KPI value, or None if the KPI is not available\n",
      "        \"\"\"\n",
      "        if kpi_name not in KPIManagerModified.supported_kpis:\n",
      "            self.log_warning(\"KPI does not exist: \"+kpi_name)\n",
      "            return None\n",
      "\n",
      "        kpi_agent = self.get_analyzer(KPIManagerModified.supported_kpis[kpi_name])\n",
      "        if not kpi_agent:\n",
      "            # KPI analyzer not triggered\n",
      "            self.log_warning(\"KPI not activated yet: \"+kpi_name)\n",
      "            self.enable_kpi(kpi_name)\n",
      "            return None\n",
      "\n",
      "        # Modification: Log additional information for remote query\n",
      "        self.log_info(f\"Remote query for KPI: {kpi_name} at timestamp: {timestamp}\")\n",
      "        \n",
      "        return kpi_agent.local_query_kpi(kpi_name, phone_model, operator, gps, timestamp)\n",
      "\n",
      "Target Prompt:\n",
      "Prompt: I want you to define a class `KPIManagerModified` that inherits from a base `Analyzer` class, providing modified calculations and logging for KPIs:\n",
      "\n",
      "1. Class Definition: `KPIManagerModified`\n",
      "This class extends the `Analyzer` class to offer a unified interface for tracking and querying KPIs. It includes a mechanism to identify and load supported KPIs from the `mobile_insight` library.\n",
      "\n",
      "   - Initialization: The constructor initializes the base `Analyzer` class and calls a helper function `__check_kpis` which dynamically identifies supported KPI analyzers by inspecting the `mobile_insight.analyzer.kpi` module. It logs the available KPIs.\n",
      "\n",
      "   - KPI Listing: The `list_kpis` function returns a list of all available KPI names that can be monitored.\n",
      "\n",
      "   - KPI Enabling: The `enable_kpi` function allows for enabling a specific KPI by its name. It includes modifications such as logging additional information when a KPI is activated. It also allows setting a periodicity and whether to enable local storage.\n",
      "\n",
      "   - Enable All: The `enable_all_kpis` method enables monitoring for all identified KPIs.\n",
      "\n",
      "2. KPI Query Functions:\n",
      "   - Local Query: `local_query_kpi` allows querying the locally observed KPI values. It includes modified logic to adjust query behavior based on a given mode (e.g., 'cell') and logs additional information based on the query mode.\n",
      "\n",
      "   - Remote Query: `remote_query_kpi` facilitates querying KPI data from a remote cloud service, providing enhanced logging to track remote query operations.\n",
      "\n",
      "3. Functionality and Usage:\n",
      "The class offers enhanced logging and additional functionality over a traditional KPI manager, making it suitable for scenarios requiring detailed tracking and querying of KPI metrics with modified calculations and handling logic.\n",
      "# Usage: python kpi=manager-test.py [dirname]\n",
      "# Example1: python kpi-manager-test-experimental.py logs/bler_sample.mi2log \n",
      "# (For testing KPI BLER)\n",
      "# Example2: python kpi-manager-test-experimental.py logs/data_sample.mi2log \n",
      "# (For testing KPI DL_PDCP_LOSS, HANDOVER_PREDICTION, HANDOVER_LATENCY, HANDOVER_HOL)\n",
      "# import os\n",
      "import sys\n",
      "\n",
      "from mobile_insight.monitor import OfflineReplayer\n",
      "from mobile_insight.analyzer.kpi import KPIManager, KpiAnalyzer\n",
      "import cProfile\n",
      "\n",
      "\n",
      "def kpi_manager_example():\n",
      "\n",
      "    src = OfflineReplayer()\n",
      "    src.set_input_path('./logs/offline_log_examples/20201115_181637_Xiaomi-Mi10_46000.mi2log')\n",
      "\n",
      "    kpi_manager = KPIManager()\n",
      "    # print \"All supported KPIs:\", str(kpi_manager.list_kpis())\n",
      "\n",
      "    # Test experimental KPIs - data plane\n",
      "    kpi_manager.enable_kpi(\"KPI.Wireless.BLER\") # test log: bler_sample\n",
      "    kpi_manager.enable_kpi(\"KPI.Wireless.DL_PDCP_LOSS\") # test log: data_sample\n",
      "    kpi_manager.enable_kpi(\"KPI.Wireless.UL_PDCP_LOSS\")\n",
      "\n",
      "    # Test experimental KPIs - handover\n",
      "    kpi_manager.enable_kpi(\"KPI.Mobility.HANDOVER_PREDICTION\") # test log: data_sample\n",
      "    kpi_manager.enable_kpi(\"KPI.Mobility.HANDOVER_LATENCY\") # test log: data_sample\n",
      "    kpi_manager.enable_kpi(\"KPI.Mobility.HANDOVER_HOL\") # test log: data_sample\n",
      "\n",
      "    kpi_manager.set_source(src)\n",
      "\n",
      "    src.run()\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    kpi_manager_example()\n",
      "\n",
      "  - actual output: #!/usr/bin/python\n",
      "# Filename: kpi_manager_modified.py\n",
      "\"\"\"\n",
      "A modified KPI Manager that provides enhanced calculations and logging for KPIs.\n",
      "\n",
      "Author: Your Name\n",
      "\"\"\"\n",
      "\n",
      "from mobile_insight.analyzer.analyzer import *\n",
      "import importlib\n",
      "import logging\n",
      "\n",
      "__all__ = [\"KPIManagerModified\"]\n",
      "\n",
      "class KPIManagerModified(Analyzer):\n",
      "    \"\"\"\n",
      "    A modified KPI Manager for tracking and querying KPIs with enhanced functionality.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        Analyzer.__init__(self)\n",
      "        self.kpi_analyzers = {}\n",
      "        self.__check_kpis()\n",
      "\n",
      "    def __check_kpis(self):\n",
      "        \"\"\"\n",
      "        Dynamically identify supported KPI analyzers by inspecting the mobile_insight.analyzer.kpi module.\n",
      "        \"\"\"\n",
      "        try:\n",
      "            kpi_module = importlib.import_module(\"mobile_insight.analyzer.kpi\")\n",
      "            for name in dir(kpi_module):\n",
      "                kpi_class = getattr(kpi_module, name)\n",
      "                if isinstance(kpi_class, type) and issubclass(kpi_class, KpiAnalyzer) and kpi_class is not KpiAnalyzer:\n",
      "                    self.kpi_analyzers[name] = kpi_class()\n",
      "            self.log_info(\"Available KPIs: \" + \", \".join(self.kpi_analyzers.keys()))\n",
      "        except Exception as e:\n",
      "            self.log_error(\"Failed to load KPI analyzers: \" + str(e))\n",
      "\n",
      "    def list_kpis(self):\n",
      "        \"\"\"\n",
      "        Returns a list of all available KPI names that can be monitored.\n",
      "        \"\"\"\n",
      "        return list(self.kpi_analyzers.keys())\n",
      "\n",
      "    def enable_kpi(self, kpi_name, periodicity=None, enable_local_storage=False):\n",
      "        \"\"\"\n",
      "        Enable a specific KPI by its name with optional periodicity and local storage settings.\n",
      "        \"\"\"\n",
      "        if kpi_name in self.kpi_analyzers:\n",
      "            kpi_analyzer = self.kpi_analyzers[kpi_name]\n",
      "            kpi_analyzer.enable()\n",
      "            if periodicity:\n",
      "                kpi_analyzer.set_periodicity(periodicity)\n",
      "            kpi_analyzer.enable_local_storage(enable_local_storage)\n",
      "            self.log_info(f\"KPI {kpi_name} enabled with periodicity {periodicity} and local storage {enable_local_storage}\")\n",
      "        else:\n",
      "            self.log_error(f\"KPI {kpi_name} is not available\")\n",
      "\n",
      "    def enable_all_kpis(self):\n",
      "        \"\"\"\n",
      "        Enable monitoring for all identified KPIs.\n",
      "        \"\"\"\n",
      "        for kpi_name in self.kpi_analyzers.keys():\n",
      "            self.enable_kpi(kpi_name)\n",
      "\n",
      "    def local_query_kpi(self, kpi_name, mode='default'):\n",
      "        \"\"\"\n",
      "        Query the locally observed KPI values with additional logic based on the mode.\n",
      "        \"\"\"\n",
      "        if kpi_name in self.kpi_analyzers:\n",
      "            kpi_analyzer = self.kpi_analyzers[kpi_name]\n",
      "            result = kpi_analyzer.query(mode)\n",
      "            self.log_info(f\"Local query for KPI {kpi_name} with mode {mode}: {result}\")\n",
      "            return result\n",
      "        else:\n",
      "            self.log_error(f\"KPI {kpi_name} is not available for local querying\")\n",
      "            return None\n",
      "\n",
      "    def remote_query_kpi(self, kpi_name):\n",
      "        \"\"\"\n",
      "        Query KPI data from a remote cloud service with enhanced logging.\n",
      "        \"\"\"\n",
      "        if kpi_name in self.kpi_analyzers:\n",
      "            kpi_analyzer = self.kpi_analyzers[kpi_name]\n",
      "            try:\n",
      "                result = kpi_analyzer.remote_query()\n",
      "                self.log_info(f\"Remote query for KPI {kpi_name}: {result}\")\n",
      "                return result\n",
      "            except Exception as e:\n",
      "                self.log_error(f\"Failed to perform remote query for KPI {kpi_name}: {str(e)}\")\n",
      "                return None\n",
      "        else:\n",
      "            self.log_error(f\"KPI {kpi_name} is not available for remote querying\")\n",
      "            return None\n",
      "\n",
      "    def set_source(self, source):\n",
      "        \"\"\"\n",
      "        Set the trace source. Enable the KPI analyzers to use the provided source.\n",
      "        \"\"\"\n",
      "        Analyzer.set_source(self, source)\n",
      "        for kpi_analyzer in self.kpi_analyzers.values():\n",
      "            kpi_analyzer.set_source(source)\n",
      "  - expected output: #!/usr/bin/python\n",
      "# Filename: kpi_manager_modified.py\n",
      "\"\"\"\n",
      "kpi_manager_modified.py\n",
      "An unified interface for users to track and query KPIs with modified calculations\n",
      "\n",
      "Author: Yuanjie Li\n",
      "\"\"\"\n",
      "\n",
      "__all__ = [\"KPIManagerModified\"]\n",
      "\n",
      "from ..analyzer import *\n",
      "import sys, inspect, os\n",
      "\n",
      "\n",
      "class KPIManagerModified(Analyzer):\n",
      "\n",
      "    \"\"\"\n",
      "    An unified interface for users to track and query KPIs\n",
      "    \"\"\"\n",
      "\n",
      "    supported_kpis={} # Supported KPIs: kpi_name -> KPIAnalyzer name\n",
      "\n",
      "    def __init__(self):\n",
      "        Analyzer.__init__(self)\n",
      "        self.__check_kpis()\n",
      "\n",
      "\n",
      "\n",
      "    def __check_kpis(self):\n",
      "\n",
      "        \"\"\"\n",
      "        Find and include all supported KPIs into KPIManager.supported_kpis\n",
      "        \"\"\"\n",
      "        module_tmp = __import__(\"mobile_insight\")\n",
      "        for item in inspect.getmembers(module_tmp.analyzer.kpi, inspect.isclass):\n",
      "            if item[1].__bases__[0].__name__ ==  \"KpiAnalyzer\":\n",
      "                tmp_module = item[1]()\n",
      "                for kpi in tmp_module.list_kpis():\n",
      "                        KPIManagerModified.supported_kpis[kpi] = item[0]\n",
      "                        self.log_info(kpi)\n",
      "        \n",
      "\n",
      "    def list_kpis(self):\n",
      "        \"\"\"\n",
      "        Return a list of available KPIs \n",
      "\n",
      "        :returns: a list of string, each of which is a KPI name\n",
      "        \"\"\"\n",
      "        return list(self.supported_kpis.keys())\n",
      "\n",
      "    def enable_all_kpis(self, enable_storage = False):\n",
      "        \"\"\"\n",
      "        Enable all KPIs' monitoring\n",
      "        \n",
      "        :param enable_storage: Whether to locally store the kpi. False by default\n",
      "        :type enable_storage: boolean\n",
      "        \"\"\"\n",
      "        for kpi_name in self.list_kpis():\n",
      "            self.enable_kpi(kpi_name, enable_storage)\n",
      "\n",
      "\n",
      "    def enable_kpi(self, kpi_name, periodicity='0s', cell=None, enable_storage = True):\n",
      "        \"\"\"\n",
      "        Enable the KPI monitoring with slight modification\n",
      "\n",
      "        :param kpi_name: The KPI to be monitored\n",
      "        :type kpi_name: string\n",
      "        :param enable_storage: Whether to locally store the kpi. False by default\n",
      "        :type enable_storage: boolean\n",
      "        :returns: True if successfully activated, False otherwise\n",
      "        \"\"\"\n",
      "\n",
      "        if kpi_name not in self.supported_kpis:\n",
      "            self.log_warning(\"KPI does not exist: \"+kpi_name)\n",
      "            return False\n",
      "\n",
      "        try: \n",
      "            kpi_analyzer_name = self.supported_kpis[kpi_name]\n",
      "            self.include_analyzer(kpi_analyzer_name, [])\n",
      "            self.get_analyzer(kpi_analyzer_name).enable_local_storage(enable_storage)\n",
      "            self.get_analyzer(kpi_analyzer_name).set_periodicity(kpi_name, periodicity)\n",
      "            self.get_analyzer(kpi_analyzer_name).set_cell(kpi_name, cell)\n",
      "            # Modification: Log additional info for KPI activation\n",
      "            self.log_info(f\"Enable KPI: {kpi_name} with periodicity: {periodicity} and storage: {enable_storage}\")\n",
      "            return True\n",
      "        except Exception as e:\n",
      "            # Import failure\n",
      "            self.log_warning(\"Fail to activate KPI: \"+kpi_name)    \n",
      "            return False\n",
      "\n",
      "\n",
      "    def local_query_kpi(self, kpi_name, mode = 'cell', timestamp = None):\n",
      "        \"\"\"\n",
      "        Query the phone's locally observed KPI\n",
      "\n",
      "        :param kpi_name: The KPI to be queried\n",
      "        :type kpi_name: string\n",
      "        :param timestamp: The timestamp of the KPI. If None, this function returns the latest KPI\n",
      "        :type timestamp: datetime\n",
      "        :returns: The KPI value, or None if the KPI is not available\n",
      "        \"\"\"\n",
      "        if kpi_name not in self.supported_kpis:\n",
      "            self.log_warning(\"KPI does not exist: \"+kpi_name)\n",
      "            return None\n",
      "\n",
      "        kpi_agent = self.get_analyzer(self.supported_kpis[kpi_name])\n",
      "        if not kpi_agent:\n",
      "            # KPI analyzer not triggered\n",
      "            self.log_warning(\"KPI not activated yet: \"+kpi_name)\n",
      "            self.enable_kpi(kpi_name)\n",
      "            return None\n",
      "\n",
      "        # Modification: Adjust query mode logic (e.g., simulate different processing)\n",
      "        if mode == 'cell':\n",
      "            self.log_info(f\"Querying KPI: {kpi_name} in cell mode\")\n",
      "        else:\n",
      "            self.log_info(f\"Querying KPI: {kpi_name} in {mode} mode\")\n",
      "        \n",
      "        return kpi_agent.local_query_kpi(kpi_name, mode, timestamp)\n",
      "\n",
      "    def remote_query_kpi(self, kpi_name, phone_model, operator, gps, timestamp):\n",
      "        \"\"\"\n",
      "        Query the remote cloud for the KPI\n",
      "\n",
      "        :param kpi_name: The KPI to be queried\n",
      "        :type kpi_name: string\n",
      "        :param phone_model: The the phone model\n",
      "        :type phone_model: string\n",
      "        :param operator: The network operator\n",
      "        :type operator: string\n",
      "        :param gps: The GPS coordinate\n",
      "        :type gps: string\n",
      "        :param timestamp: The timestamp of the KPI. \n",
      "        :type timestamp: datetime\n",
      "        :returns: The KPI value, or None if the KPI is not available\n",
      "        \"\"\"\n",
      "        if kpi_name not in KPIManagerModified.supported_kpis:\n",
      "            self.log_warning(\"KPI does not exist: \"+kpi_name)\n",
      "            return None\n",
      "\n",
      "        kpi_agent = self.get_analyzer(KPIManagerModified.supported_kpis[kpi_name])\n",
      "        if not kpi_agent:\n",
      "            # KPI analyzer not triggered\n",
      "            self.log_warning(\"KPI not activated yet: \"+kpi_name)\n",
      "            self.enable_kpi(kpi_name)\n",
      "            return None\n",
      "\n",
      "        # Modification: Log additional information for remote query\n",
      "        self.log_info(f\"Remote query for KPI: {kpi_name} at timestamp: {timestamp}\")\n",
      "        \n",
      "        return kpi_agent.local_query_kpi(kpi_name, phone_model, operator, gps, timestamp)\n",
      "  - context: []\n",
      "  - retrieval context: []\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Hallucination: 100.00% pass rate\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Correctness (GEval): 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import HallucinationMetric, AnswerRelevancyMetric, GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "\n",
    "hallucination_metric = HallucinationMetric(threshold=0.3)\n",
    "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n",
    "correctness_metric = GEval(\n",
    "        threshold=0.65,\n",
    "        name=\"Correctness\",\n",
    "        evaluation_steps=[\n",
    "            \"Check whether the code logic in 'actual output' contradict any code logic in 'expected output'\",\n",
    "            \"Heavily penalize misuse of imports and non-existant functions\",\n",
    "            \"different variable names and different structure are okay\"\n",
    "        ],\n",
    "        evaluation_params=[\n",
    "          LLMTestCaseParams.INPUT,\n",
    "          LLMTestCaseParams.ACTUAL_OUTPUT,\n",
    "          LLMTestCaseParams.EXPECTED_OUTPUT\n",
    "        ],\n",
    "        model=\"gpt-4o\"\n",
    "    )\n",
    "\n",
    "# dataset.evaluate([hallucination_metric, answer_relevancy_metric])\n",
    "\n",
    "# You can also call the evaluate() function directly\n",
    "evaluation_results = evaluate(test_cases=inner_dataset_baseline, metrics=[hallucination_metric, answer_relevancy_metric, correctness_metric], \\\n",
    "                              run_async=False, max_concurrent=1)\n",
    "# run_async=False, max_concurrent=1, throttle_value=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_results': [{'name': 'test_case_0',\n",
       "   'success': True,\n",
       "   'metrics_data': [{'name': 'Hallucination',\n",
       "     'threshold': 0.3,\n",
       "     'success': True,\n",
       "     'score': 0.0,\n",
       "     'reason': 'The score is 0.00 because there are no factual alignments or contradictions, indicating that the actual output is perfectly aligned with the contexts provided.',\n",
       "     'strict_mode': False,\n",
       "     'evaluation_model': 'gpt-4o',\n",
       "     'error': None,\n",
       "     'evaluation_cost': 0.0039975,\n",
       "     'verbose_logs': 'Verdicts:\\n[]'},\n",
       "    {'name': 'Answer Relevancy',\n",
       "     'threshold': 0.5,\n",
       "     'success': True,\n",
       "     'score': 0.7096774193548387,\n",
       "     'reason': \"The score is 0.71 because the output contains relevant code for the KPIManagerModified class with appropriate methods for KPI management. However, the presence of multiple irrelevant comments, author placeholders, and other non-functional elements slightly detracts from the focus on the code's functionality, which prevents the score from being higher.\",\n",
       "     'strict_mode': False,\n",
       "     'evaluation_model': 'gpt-4o',\n",
       "     'error': None,\n",
       "     'evaluation_cost': 0.05708250000000001,\n",
       "     'verbose_logs': 'Statements:\\n[\\n    \"#!/usr/bin/python\",\\n    \"# Filename: kpi_manager_modified.py\",\\n    \"\"\"\"\",\\n    \"A modified KPI Manager that provides enhanced calculations and logging for KPIs.\",\\n    \"Author: Your Name\",\\n    \"\"\"\"\",\\n    \"from mobile_insight.analyzer.analyzer import *\",\\n    \"import importlib\",\\n    \"import logging\",\\n    \"__all__ = [\"KPIManagerModified\"]\",\\n    \"class KPIManagerModified(Analyzer):\",\\n    \"\"\"\"\",\\n    \"A modified KPI Manager for tracking and querying KPIs with enhanced functionality.\",\\n    \"\"\"\"\",\\n    \"def __init__(self):\",\\n    \"Analyzer.__init__(self)\",\\n    \"self.kpi_analyzers = {}\",\\n    \"self.__check_kpis()\",\\n    \"def __check_kpis(self):\",\\n    \"\"\"\"\",\\n    \"Dynamically identify supported KPI analyzers by inspecting the mobile_insight.analyzer.kpi module.\",\\n    \"\"\"\"\",\\n    \"try:\",\\n    \"kpi_module = importlib.import_module(\"mobile_insight.analyzer.kpi\")\",\\n    \"for name in dir(kpi_module):\",\\n    \"kpi_class = getattr(kpi_module, name)\",\\n    \"if isinstance(kpi_class, type) and issubclass(kpi_class, KpiAnalyzer) and kpi_class is not KpiAnalyzer:\",\\n    \"self.kpi_analyzers[name] = kpi_class()\",\\n    \"self.log_info(\"Available KPIs: \" + \", \".join(self.kpi_analyzers.keys()))\",\\n    \"except Exception as e:\",\\n    \"self.log_error(\"Failed to load KPI analyzers: \" + str(e))\",\\n    \"def list_kpis(self):\",\\n    \"\"\"\"\",\\n    \"Returns a list of all available KPI names that can be monitored.\",\\n    \"\"\"\"\",\\n    \"return list(self.kpi_analyzers.keys())\",\\n    \"def enable_kpi(self, kpi_name, periodicity=None, enable_local_storage=False):\",\\n    \"\"\"\"\",\\n    \"Enable a specific KPI by its name with optional periodicity and local storage settings.\",\\n    \"\"\"\"\",\\n    \"if kpi_name in self.kpi_analyzers:\",\\n    \"kpi_analyzer = self.kpi_analyzers[kpi_name]\",\\n    \"kpi_analyzer.enable()\",\\n    \"if periodicity:\",\\n    \"kpi_analyzer.set_periodicity(periodicity)\",\\n    \"kpi_analyzer.enable_local_storage(enable_local_storage)\",\\n    \"self.log_info(f\"KPI {kpi_name} enabled with periodicity {periodicity} and local storage {enable_local_storage}\")\",\\n    \"else:\",\\n    \"self.log_error(f\"KPI {kpi_name} is not available\")\",\\n    \"def enable_all_kpis(self):\",\\n    \"\"\"\"\",\\n    \"Enable monitoring for all identified KPIs.\",\\n    \"\"\"\"\",\\n    \"for kpi_name in self.kpi_analyzers.keys():\",\\n    \"self.enable_kpi(kpi_name)\",\\n    \"def local_query_kpi(self, kpi_name, mode=\\'default\\'):\",\\n    \"\"\"\"\",\\n    \"Query the locally observed KPI values with additional logic based on the mode.\",\\n    \"\"\"\"\",\\n    \"if kpi_name in self.kpi_analyzers:\",\\n    \"kpi_analyzer = self.kpi_analyzers[kpi_name]\",\\n    \"result = kpi_analyzer.query(mode)\",\\n    \"self.log_info(f\"Local query for KPI {kpi_name} with mode {mode}: {result}\")\",\\n    \"return result\",\\n    \"else:\",\\n    \"self.log_error(f\"KPI {kpi_name} is not available for local querying\")\",\\n    \"return None\",\\n    \"def remote_query_kpi(self, kpi_name):\",\\n    \"\"\"\"\",\\n    \"Query KPI data from a remote cloud service with enhanced logging.\",\\n    \"\"\"\"\",\\n    \"if kpi_name in self.kpi_analyzers:\",\\n    \"kpi_analyzer = self.kpi_analyzers[kpi_name]\",\\n    \"try:\",\\n    \"result = kpi_analyzer.remote_query()\",\\n    \"self.log_info(f\"Remote query for KPI {kpi_name}: {result}\")\",\\n    \"return result\",\\n    \"except Exception as e:\",\\n    \"self.log_error(f\"Failed to perform remote query for KPI {kpi_name}: {str(e)}\")\",\\n    \"return None\",\\n    \"else:\",\\n    \"self.log_error(f\"KPI {kpi_name} is not available for remote querying\")\",\\n    \"return None\",\\n    \"def set_source(self, source):\",\\n    \"\"\"\"\",\\n    \"Set the trace source. Enable the KPI analyzers to use the provided source.\",\\n    \"\"\"\"\",\\n    \"Analyzer.set_source(self, source)\",\\n    \"for kpi_analyzer in self.kpi_analyzers.values():\",\\n    \"kpi_analyzer.set_source(source)\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This statement is a shebang line, which is not directly relevant to the functionality of the KPIManagerModified class or its code implementation.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This statement is a comment indicating the filename, which is not directly relevant to the functionality of the KPIManagerModified class or its code implementation.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This is an opening triple-quote for a comment or docstring, which does not provide useful information on its own about the KPIManagerModified class.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This statement is a comment describing the KPI Manager, which is not part of the code logic itself but serves as documentation.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This statement contains an author name placeholder, which is not relevant to the code\\'s functionality.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This is a closing triple-quote for a comment or docstring, which does not provide useful information on its own about the KPIManagerModified class.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This is an opening triple-quote for a comment or docstring, which does not provide useful information on its own about the KPIManagerModified class.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This statement is a comment describing the KPI Manager, which is not part of the code logic itself but serves as documentation.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This is a closing triple-quote for a comment or docstring, which does not provide useful information on its own about the KPIManagerModified class.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This is an opening triple-quote for a comment or docstring, which does not provide useful information on its own about the KPIManagerModified class.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This statement is a comment describing the purpose of the method, which is not part of the code logic itself but serves as documentation.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This is a closing triple-quote for a comment or docstring, which does not provide useful information on its own about the KPIManagerModified class.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This statement is a comment describing the purpose of the method, which is not part of the code logic itself but serves as documentation.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This is a closing triple-quote for a comment or docstring, which does not provide useful information on its own about the KPIManagerModified class.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This statement is a comment describing the purpose of the method, which is not part of the code logic itself but serves as documentation.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This is a closing triple-quote for a comment or docstring, which does not provide useful information on its own about the KPIManagerModified class.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This statement is a comment describing the purpose of the method, which is not part of the code logic itself but serves as documentation.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This is a closing triple-quote for a comment or docstring, which does not provide useful information on its own about the KPIManagerModified class.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]'},\n",
       "    {'name': 'Correctness (GEval)',\n",
       "     'threshold': 0.65,\n",
       "     'success': True,\n",
       "     'score': 0.6607769200686414,\n",
       "     'reason': \"The actual output aligns well with the expected output in terms of class structure and functionality, including methods for enabling KPIs and querying them. However, it deviates by using importlib and logging instead of inspect and os, and it lacks some expected parameters in methods like 'remote_query_kpi'. It also does not include 'cell' as a parameter in 'enable_kpi'.\",\n",
       "     'strict_mode': False,\n",
       "     'evaluation_model': 'gpt-4o',\n",
       "     'error': None,\n",
       "     'evaluation_cost': 0.0193,\n",
       "     'verbose_logs': 'Evaluation Steps:\\n[\\n    \"Check whether the code logic in \\'actual output\\' contradict any code logic in \\'expected output\\'\",\\n    \"Heavily penalize misuse of imports and non-existant functions\",\\n    \"different variable names and different structure are okay\"\\n]'}],\n",
       "   'conversational': False,\n",
       "   'multimodal': False,\n",
       "   'input': '\\n        You are an AI assistant that generates code for inner analyzers using the Mobileinsight-core Python library, a library that enables below-IP,         fine-grained mobile network analytics on end devices. It is a cross-platform package for mobile network monitoring and analysis.\\n\\n        For context, I will be giving a few examples of a prompt + outer analyzer code pairs along with their corresponding expected inner analyzer code.\\n\\n        Then I will give the main target prompt that you need to follow in order to generate an inner analyzer.\\n\\n        NOTE: PLEASE PROVIDE ONLY THE CODE AND NOTHING ELSE, AS THIS OUTPUT IS BEING DIRECTLY SAVED TO A .PY FILE AND RAN AUTONOMOUSLY.         ADDITIONALLY, ENSURE THAT YOU PROVIDE THE FULL COMPLETE CODE, AND DO NOT LEAVE OUT ANY PARTS FOR THE USER TO COMPLETE. THE CODE SHOULD FULLY RUN         WITH NO ADDITIONAL MODIFICATIONS REQUIRED.\\n        Example 1:\\nPrompt: I want you to define a class `ModemDebugAnalyzerModified` that inherits from a base `Analyzer` class, and processes modem debug messages to extract specific metrics:\\n\\n1. Class Definition: `ModemDebugAnalyzerModified`\\nThis class extends from the base `Analyzer` class. It configures the source by enabling logs for \"Modem_debug_message\". It processes these messages through the `__msg_callback` function, which decodes incoming messages and performs additional analyses:\\n   - Logs the original modem debug message.\\n   - Computes and logs the word count of the message.\\n   - Checks for the presence of the keyword \\'Error\\' within the message and logs its detection.\\n\\n2. Integration with Outer Analyzer: \\nThe class will be integrated into an outer analyzer script, which utilizes the `ModemDebugAnalyzerModified` class to evaluate metrics from the replayed logs. \\n\\n3. Execution Logic:\\nThe outer analyzer will set the input path for the log files, initialize the `ModemDebugAnalyzerModified` class, and configure it with an `OfflineReplayer` as the data source. The analysis is executed by replaying the logs, processing each message to extract and log the specified metrics, and saving the results to a specified output file. The execution should be robust, handling any potential exceptions during log replay and analysis.\\n#!/usr/bin/python\\n# Filename: offline-analysis-example.py\\nimport os\\nimport sys\\n\\n\"\"\"\\nOffline analysis by replaying logs\\n\"\"\"\\n\\n# Import MobileInsight modules\\nfrom mobile_insight.monitor import OfflineReplayer\\nfrom mobile_insight.analyzer import MsgLogger, ModemDebugAnalyzer\\nif __name__ == \"__main__\":\\n\\n    # Initialize a monitor\\n    src = OfflineReplayer()\\n    src.set_input_path(\"./logs/\")\\n    # src.enable_log_all()\\n\\n    src.enable_log(\"LTE_PHY_Serv_Cell_Measurement\")\\n    src.enable_log(\"5G_NR_RRC_OTA_Packet\")\\n    src.enable_log(\"LTE_RRC_OTA_Packet\")\\n    src.enable_log(\"LTE_NB1_ML1_GM_DCI_Info\")\\n\\n    logger = MsgLogger()\\n    logger.set_decode_format(MsgLogger.XML)\\n    logger.set_dump_type(MsgLogger.FILE_ONLY)\\n    logger.save_decoded_msg_as(\"./test.txt\")\\n    logger.set_source(src)\\n\\n    modem_debug_analyzer = ModemDebugAnalyzer()\\n    modem_debug_analyzer.set_source(src)\\n\\n    # Start the monitoring\\n    src.run()\\n\\n Expected Output:\\n#!/usr/bin/python\\n# Filename: modem_debug_analyzer_modified.py\\n\"\"\"\\nA modified debugger for cellular interface with additional metrics\\n\\nAuthor: Yuanjie Li (Modified)\\n\"\"\"\\n\\nfrom mobile_insight.analyzer.analyzer import *\\n\\n__all__ = [\"ModemDebugAnalyzerModified\"]\\n\\nclass ModemDebugAnalyzerModified(Analyzer):\\n\\n    def __init__(self):\\n        Analyzer.__init__(self)\\n\\n        self.add_source_callback(self.__msg_callback)\\n\\n    def set_source(self, source):\\n        \"\"\"\\n        Set the trace source. Enable the cellular signaling messages\\n\\n        :param source: the trace source (collector).\\n        \"\"\"\\n        Analyzer.set_source(self, source)\\n\\n        # Phy-layer logs\\n        source.enable_log(\"Modem_debug_message\")\\n\\n    def __msg_callback(self, msg):\\n\\n        if msg.type_id == \"Modem_debug_message\":\\n\\n            log_item = msg.data.decode()\\n\\n            if \\'Msg\\' in log_item:\\n                # Log the original message\\n                self.log_info(log_item[\"Msg\"])\\n\\n                # Additional metric: count the number of words in the message\\n                word_count = len(log_item[\"Msg\"].split())\\n                self.log_info(f\"Word count in message: {word_count}\")\\n\\n                # Additional metric: check if \\'Error\\' keyword is in the message\\n                if \\'Error\\' in log_item[\"Msg\"]:\\n                    self.log_info(\"Error keyword detected in message.\")\\n\\nExample 2:\\nPrompt: I want you to define a class `MsgStatisticsModified` that inherits from a base `Analyzer` class, and returns statistics for cellular messages, including message type counts, arrival intervals, and average message lengths:\\n\\n1. Class Definition: `MsgStatisticsModified`\\nThis class extends from a base `Analyzer` class. It should initialize and maintain dictionaries to store message type statistics, arrival intervals, lengths, and average lengths. The `set_source` method sets the trace source and enables all cellular signaling messages.\\n\\n2. Message Processing: `__msg_callback`\\nThe `__msg_callback` function processes each message to update the statistics:\\n   - For each message, update the count of the message type.\\n   - Record the timestamp for arrival intervals.\\n   - Capture the message length from fields like `log_msg_len`, `Msg Length`, or `Message Length`.\\n   - Calculate the average message length for each message type.\\n\\n3. Reset Functionality: `reset`\\nInclude a `reset` method to clear all statistics, allowing the analyzer to be reused for different analysis sessions.\\n\\nThis class will be used by the outer analyzer file to evaluate metrics such as message type statistics, arrival intervals, and average message lengths from offline log data.\\n#!/usr/bin/python\\n# Filename: msg-statistics-example.py\\nimport os\\nimport sys\\n\\n# Import MobileInsight modules\\nfrom mobile_insight.monitor import OfflineReplayer\\nfrom mobile_insight.analyzer.msg_statistics import MsgStatistics\\n\\n\"\"\"\\nThis example shows how to get basic statistics of a offline log\\n\"\"\"\\nif __name__ == \"__main__\":\\n\\n    # Initialize a 3G/4G monitor\\n    src = OfflineReplayer()\\n    src.set_input_path(\"./offline_log_example.mi2log\")\\n\\n    statistics = MsgStatistics()\\n    statistics.set_source(src)\\n\\n    # Start the monitoring\\n    src.run()\\n\\n    # Save results\\n    f_statistics = open(\\'./msg_type_statistics.txt\\', \\'w\\')\\n    for item in statistics.msg_type_statistics:\\n        f_statistics.write(\\n            item + \" \" + str(statistics.msg_type_statistics[item]) + \"\\\\n\")\\n    f_statistics.close()\\n\\n    f_rate = open(\\'./msg_arrival_rate.txt\\', \\'w\\')\\n    for item in statistics.msg_arrival_rate:\\n        f_rate.write(item + \" \")\\n        for k in range(1, len(statistics.msg_arrival_rate[item])):\\n            f_rate.write(str(\\n                (statistics.msg_arrival_rate[item][k] - statistics.msg_arrival_rate[item][k - 1]).total_seconds() * 1000) + \" \")\\n        f_rate.write(\"\\\\n\")\\n    f_rate.close()\\n\\n    f_msg_len = open(\\'./msg_length.txt\\', \\'w\\')\\n    for item in statistics.msg_lengh:\\n        f_msg_len.write(item + \" \")\\n        for k in range(0, len(statistics.msg_lengh[item])):\\n            f_msg_len.write(str(statistics.msg_lengh[item][k]) + \" \")\\n        f_msg_len.write(\"\\\\n\")\\n    f_msg_len.close()\\n\\n Expected Output:\\n#!/usr/bin/python\\n# Filename: msg_statistics_modified.py\\n\"\"\"\\nA modified analyzer to study the cellular message statistics, arrival interval time,\\nand calculate the average message length\\n\\nAuthor: Yuanjie Li\\n\"\"\"\\n\\n\\nfrom mobile_insight.analyzer.analyzer import *\\n\\n__all__ = [\"MsgStatisticsModified\"]\\n\\n\\nclass MsgStatisticsModified(Analyzer):\\n\\n    def __init__(self):\\n        Analyzer.__init__(self)\\n\\n        self.add_source_callback(self.__msg_callback)\\n\\n        self.msg_type_statistics = {}  # type_id->msg_count\\n\\n        self.msg_arrival_rate = {}  # type_id->list of arrival interval\\n\\n        self.msg_lengh = {}  # type_id->list of message length\\n\\n        self.msg_avg_length = {}  # type_id->average message length\\n\\n    def reset(self):\\n        self.msg_type_statistics = {}  # type_id->msg_count\\n\\n        self.msg_arrival_rate = {}  # type_id->list of arrival interval\\n\\n        self.msg_lengh = {}  # type_id->list of message length\\n\\n        self.msg_avg_length = {}  # type_id->average message length\\n\\n    def set_source(self, source):\\n        \"\"\"\\n        Set the trace source. Enable the cellular signaling messages\\n\\n        :param source: the trace source (collector).\\n        \"\"\"\\n        Analyzer.set_source(self, source)\\n        source.enable_log_all()\\n\\n    def __msg_callback(self, msg):\\n\\n        log_item = msg.data.decode()\\n\\n        if msg.type_id not in self.msg_type_statistics:\\n            self.msg_type_statistics[msg.type_id] = 1\\n        else:\\n            self.msg_type_statistics[msg.type_id] = self.msg_type_statistics[msg.type_id] + 1\\n\\n        if msg.type_id not in self.msg_arrival_rate:\\n            self.msg_arrival_rate[msg.type_id] = [log_item[\"timestamp\"]]\\n        else:\\n            self.msg_arrival_rate[msg.type_id].append(log_item[\"timestamp\"])\\n\\n        if msg.type_id not in self.msg_lengh:\\n            if \"log_msg_len\" in log_item:\\n                self.msg_lengh[msg.type_id] = [log_item[\"log_msg_len\"]]\\n            elif \"Msg Length\" in log_item:\\n                self.msg_lengh[msg.type_id] = [log_item[\"Msg Length\"]]\\n            elif \"Message Length\" in log_item:\\n                self.msg_lengh[msg.type_id] = [log_item[\"Message Length\"]]\\n        else:\\n            if \"log_msg_len\" in log_item:\\n                self.msg_lengh[msg.type_id].append(log_item[\"log_msg_len\"])\\n            elif \"Msg Length\" in log_item:\\n                self.msg_lengh[msg.type_id].append(log_item[\"Msg Length\"])\\n            elif \"Message Length\" in log_item:\\n                self.msg_lengh[msg.type_id].append(log_item[\"Message Length\"])\\n\\n        # Calculate average message length\\n        if msg.type_id in self.msg_lengh:\\n            total_length = sum(self.msg_lengh[msg.type_id])\\n            count = len(self.msg_lengh[msg.type_id])\\n            self.msg_avg_length[msg.type_id] = total_length / count if count > 0 else 0\\n\\nExample 3:\\nPrompt: I want you to define a class `KPIManagerModified` that inherits from a base `Analyzer` class, providing modified calculations and logging for KPIs:\\n\\n1. Class Definition: `KPIManagerModified`\\nThis class extends the `Analyzer` class to offer a unified interface for tracking and querying KPIs. It includes a mechanism to identify and load supported KPIs from the `mobile_insight` library.\\n\\n   - Initialization: The constructor initializes the base `Analyzer` class and calls a helper function `__check_kpis` which dynamically identifies supported KPI analyzers by inspecting the `mobile_insight.analyzer.kpi` module. It logs the available KPIs.\\n\\n   - KPI Listing: The `list_kpis` function returns a list of all available KPI names that can be monitored.\\n\\n   - KPI Enabling: The `enable_kpi` function allows for enabling a specific KPI by its name. It includes modifications such as logging additional information when a KPI is activated. It also allows setting a periodicity and whether to enable local storage.\\n\\n   - Enable All: The `enable_all_kpis` method enables monitoring for all identified KPIs.\\n\\n2. KPI Query Functions:\\n   - Local Query: `local_query_kpi` allows querying the locally observed KPI values. It includes modified logic to adjust query behavior based on a given mode (e.g., \\'cell\\') and logs additional information based on the query mode.\\n\\n   - Remote Query: `remote_query_kpi` facilitates querying KPI data from a remote cloud service, providing enhanced logging to track remote query operations.\\n\\n3. Functionality and Usage:\\nThe class offers enhanced logging and additional functionality over a traditional KPI manager, making it suitable for scenarios requiring detailed tracking and querying of KPI metrics with modified calculations and handling logic.\\n# Usage: python kpi=manager-test.py [dirname]\\n# Example1: python kpi-manager-test-experimental.py logs/bler_sample.mi2log \\n# (For testing KPI BLER)\\n# Example2: python kpi-manager-test-experimental.py logs/data_sample.mi2log \\n# (For testing KPI DL_PDCP_LOSS, HANDOVER_PREDICTION, HANDOVER_LATENCY, HANDOVER_HOL)\\n# import os\\nimport sys\\n\\nfrom mobile_insight.monitor import OfflineReplayer\\nfrom mobile_insight.analyzer.kpi import KPIManager, KpiAnalyzer\\nimport cProfile\\n\\n\\ndef kpi_manager_example():\\n\\n    src = OfflineReplayer()\\n    src.set_input_path(\\'./logs/offline_log_examples/20201115_181637_Xiaomi-Mi10_46000.mi2log\\')\\n\\n    kpi_manager = KPIManager()\\n    # print \"All supported KPIs:\", str(kpi_manager.list_kpis())\\n\\n    # Test experimental KPIs - data plane\\n    kpi_manager.enable_kpi(\"KPI.Wireless.BLER\") # test log: bler_sample\\n    kpi_manager.enable_kpi(\"KPI.Wireless.DL_PDCP_LOSS\") # test log: data_sample\\n    kpi_manager.enable_kpi(\"KPI.Wireless.UL_PDCP_LOSS\")\\n\\n    # Test experimental KPIs - handover\\n    kpi_manager.enable_kpi(\"KPI.Mobility.HANDOVER_PREDICTION\") # test log: data_sample\\n    kpi_manager.enable_kpi(\"KPI.Mobility.HANDOVER_LATENCY\") # test log: data_sample\\n    kpi_manager.enable_kpi(\"KPI.Mobility.HANDOVER_HOL\") # test log: data_sample\\n\\n    kpi_manager.set_source(src)\\n\\n    src.run()\\n\\n\\nif __name__ == \\'__main__\\':\\n    kpi_manager_example()\\n\\n Expected Output:\\n#!/usr/bin/python\\n# Filename: kpi_manager_modified.py\\n\"\"\"\\nkpi_manager_modified.py\\nAn unified interface for users to track and query KPIs with modified calculations\\n\\nAuthor: Yuanjie Li\\n\"\"\"\\n\\n__all__ = [\"KPIManagerModified\"]\\n\\nfrom ..analyzer import *\\nimport sys, inspect, os\\n\\n\\nclass KPIManagerModified(Analyzer):\\n\\n    \"\"\"\\n    An unified interface for users to track and query KPIs\\n    \"\"\"\\n\\n    supported_kpis={} # Supported KPIs: kpi_name -> KPIAnalyzer name\\n\\n    def __init__(self):\\n        Analyzer.__init__(self)\\n        self.__check_kpis()\\n\\n\\n\\n    def __check_kpis(self):\\n\\n        \"\"\"\\n        Find and include all supported KPIs into KPIManager.supported_kpis\\n        \"\"\"\\n        module_tmp = __import__(\"mobile_insight\")\\n        for item in inspect.getmembers(module_tmp.analyzer.kpi, inspect.isclass):\\n            if item[1].__bases__[0].__name__ ==  \"KpiAnalyzer\":\\n                tmp_module = item[1]()\\n                for kpi in tmp_module.list_kpis():\\n                        KPIManagerModified.supported_kpis[kpi] = item[0]\\n                        self.log_info(kpi)\\n        \\n\\n    def list_kpis(self):\\n        \"\"\"\\n        Return a list of available KPIs \\n\\n        :returns: a list of string, each of which is a KPI name\\n        \"\"\"\\n        return list(self.supported_kpis.keys())\\n\\n    def enable_all_kpis(self, enable_storage = False):\\n        \"\"\"\\n        Enable all KPIs\\' monitoring\\n        \\n        :param enable_storage: Whether to locally store the kpi. False by default\\n        :type enable_storage: boolean\\n        \"\"\"\\n        for kpi_name in self.list_kpis():\\n            self.enable_kpi(kpi_name, enable_storage)\\n\\n\\n    def enable_kpi(self, kpi_name, periodicity=\\'0s\\', cell=None, enable_storage = True):\\n        \"\"\"\\n        Enable the KPI monitoring with slight modification\\n\\n        :param kpi_name: The KPI to be monitored\\n        :type kpi_name: string\\n        :param enable_storage: Whether to locally store the kpi. False by default\\n        :type enable_storage: boolean\\n        :returns: True if successfully activated, False otherwise\\n        \"\"\"\\n\\n        if kpi_name not in self.supported_kpis:\\n            self.log_warning(\"KPI does not exist: \"+kpi_name)\\n            return False\\n\\n        try: \\n            kpi_analyzer_name = self.supported_kpis[kpi_name]\\n            self.include_analyzer(kpi_analyzer_name, [])\\n            self.get_analyzer(kpi_analyzer_name).enable_local_storage(enable_storage)\\n            self.get_analyzer(kpi_analyzer_name).set_periodicity(kpi_name, periodicity)\\n            self.get_analyzer(kpi_analyzer_name).set_cell(kpi_name, cell)\\n            # Modification: Log additional info for KPI activation\\n            self.log_info(f\"Enable KPI: {kpi_name} with periodicity: {periodicity} and storage: {enable_storage}\")\\n            return True\\n        except Exception as e:\\n            # Import failure\\n            self.log_warning(\"Fail to activate KPI: \"+kpi_name)    \\n            return False\\n\\n\\n    def local_query_kpi(self, kpi_name, mode = \\'cell\\', timestamp = None):\\n        \"\"\"\\n        Query the phone\\'s locally observed KPI\\n\\n        :param kpi_name: The KPI to be queried\\n        :type kpi_name: string\\n        :param timestamp: The timestamp of the KPI. If None, this function returns the latest KPI\\n        :type timestamp: datetime\\n        :returns: The KPI value, or None if the KPI is not available\\n        \"\"\"\\n        if kpi_name not in self.supported_kpis:\\n            self.log_warning(\"KPI does not exist: \"+kpi_name)\\n            return None\\n\\n        kpi_agent = self.get_analyzer(self.supported_kpis[kpi_name])\\n        if not kpi_agent:\\n            # KPI analyzer not triggered\\n            self.log_warning(\"KPI not activated yet: \"+kpi_name)\\n            self.enable_kpi(kpi_name)\\n            return None\\n\\n        # Modification: Adjust query mode logic (e.g., simulate different processing)\\n        if mode == \\'cell\\':\\n            self.log_info(f\"Querying KPI: {kpi_name} in cell mode\")\\n        else:\\n            self.log_info(f\"Querying KPI: {kpi_name} in {mode} mode\")\\n        \\n        return kpi_agent.local_query_kpi(kpi_name, mode, timestamp)\\n\\n    def remote_query_kpi(self, kpi_name, phone_model, operator, gps, timestamp):\\n        \"\"\"\\n        Query the remote cloud for the KPI\\n\\n        :param kpi_name: The KPI to be queried\\n        :type kpi_name: string\\n        :param phone_model: The the phone model\\n        :type phone_model: string\\n        :param operator: The network operator\\n        :type operator: string\\n        :param gps: The GPS coordinate\\n        :type gps: string\\n        :param timestamp: The timestamp of the KPI. \\n        :type timestamp: datetime\\n        :returns: The KPI value, or None if the KPI is not available\\n        \"\"\"\\n        if kpi_name not in KPIManagerModified.supported_kpis:\\n            self.log_warning(\"KPI does not exist: \"+kpi_name)\\n            return None\\n\\n        kpi_agent = self.get_analyzer(KPIManagerModified.supported_kpis[kpi_name])\\n        if not kpi_agent:\\n            # KPI analyzer not triggered\\n            self.log_warning(\"KPI not activated yet: \"+kpi_name)\\n            self.enable_kpi(kpi_name)\\n            return None\\n\\n        # Modification: Log additional information for remote query\\n        self.log_info(f\"Remote query for KPI: {kpi_name} at timestamp: {timestamp}\")\\n        \\n        return kpi_agent.local_query_kpi(kpi_name, phone_model, operator, gps, timestamp)\\n\\nTarget Prompt:\\nPrompt: I want you to define a class `KPIManagerModified` that inherits from a base `Analyzer` class, providing modified calculations and logging for KPIs:\\n\\n1. Class Definition: `KPIManagerModified`\\nThis class extends the `Analyzer` class to offer a unified interface for tracking and querying KPIs. It includes a mechanism to identify and load supported KPIs from the `mobile_insight` library.\\n\\n   - Initialization: The constructor initializes the base `Analyzer` class and calls a helper function `__check_kpis` which dynamically identifies supported KPI analyzers by inspecting the `mobile_insight.analyzer.kpi` module. It logs the available KPIs.\\n\\n   - KPI Listing: The `list_kpis` function returns a list of all available KPI names that can be monitored.\\n\\n   - KPI Enabling: The `enable_kpi` function allows for enabling a specific KPI by its name. It includes modifications such as logging additional information when a KPI is activated. It also allows setting a periodicity and whether to enable local storage.\\n\\n   - Enable All: The `enable_all_kpis` method enables monitoring for all identified KPIs.\\n\\n2. KPI Query Functions:\\n   - Local Query: `local_query_kpi` allows querying the locally observed KPI values. It includes modified logic to adjust query behavior based on a given mode (e.g., \\'cell\\') and logs additional information based on the query mode.\\n\\n   - Remote Query: `remote_query_kpi` facilitates querying KPI data from a remote cloud service, providing enhanced logging to track remote query operations.\\n\\n3. Functionality and Usage:\\nThe class offers enhanced logging and additional functionality over a traditional KPI manager, making it suitable for scenarios requiring detailed tracking and querying of KPI metrics with modified calculations and handling logic.\\n# Usage: python kpi=manager-test.py [dirname]\\n# Example1: python kpi-manager-test-experimental.py logs/bler_sample.mi2log \\n# (For testing KPI BLER)\\n# Example2: python kpi-manager-test-experimental.py logs/data_sample.mi2log \\n# (For testing KPI DL_PDCP_LOSS, HANDOVER_PREDICTION, HANDOVER_LATENCY, HANDOVER_HOL)\\n# import os\\nimport sys\\n\\nfrom mobile_insight.monitor import OfflineReplayer\\nfrom mobile_insight.analyzer.kpi import KPIManager, KpiAnalyzer\\nimport cProfile\\n\\n\\ndef kpi_manager_example():\\n\\n    src = OfflineReplayer()\\n    src.set_input_path(\\'./logs/offline_log_examples/20201115_181637_Xiaomi-Mi10_46000.mi2log\\')\\n\\n    kpi_manager = KPIManager()\\n    # print \"All supported KPIs:\", str(kpi_manager.list_kpis())\\n\\n    # Test experimental KPIs - data plane\\n    kpi_manager.enable_kpi(\"KPI.Wireless.BLER\") # test log: bler_sample\\n    kpi_manager.enable_kpi(\"KPI.Wireless.DL_PDCP_LOSS\") # test log: data_sample\\n    kpi_manager.enable_kpi(\"KPI.Wireless.UL_PDCP_LOSS\")\\n\\n    # Test experimental KPIs - handover\\n    kpi_manager.enable_kpi(\"KPI.Mobility.HANDOVER_PREDICTION\") # test log: data_sample\\n    kpi_manager.enable_kpi(\"KPI.Mobility.HANDOVER_LATENCY\") # test log: data_sample\\n    kpi_manager.enable_kpi(\"KPI.Mobility.HANDOVER_HOL\") # test log: data_sample\\n\\n    kpi_manager.set_source(src)\\n\\n    src.run()\\n\\n\\nif __name__ == \\'__main__\\':\\n    kpi_manager_example()\\n',\n",
       "   'actual_output': '#!/usr/bin/python\\n# Filename: kpi_manager_modified.py\\n\"\"\"\\nA modified KPI Manager that provides enhanced calculations and logging for KPIs.\\n\\nAuthor: Your Name\\n\"\"\"\\n\\nfrom mobile_insight.analyzer.analyzer import *\\nimport importlib\\nimport logging\\n\\n__all__ = [\"KPIManagerModified\"]\\n\\nclass KPIManagerModified(Analyzer):\\n    \"\"\"\\n    A modified KPI Manager for tracking and querying KPIs with enhanced functionality.\\n    \"\"\"\\n\\n    def __init__(self):\\n        Analyzer.__init__(self)\\n        self.kpi_analyzers = {}\\n        self.__check_kpis()\\n\\n    def __check_kpis(self):\\n        \"\"\"\\n        Dynamically identify supported KPI analyzers by inspecting the mobile_insight.analyzer.kpi module.\\n        \"\"\"\\n        try:\\n            kpi_module = importlib.import_module(\"mobile_insight.analyzer.kpi\")\\n            for name in dir(kpi_module):\\n                kpi_class = getattr(kpi_module, name)\\n                if isinstance(kpi_class, type) and issubclass(kpi_class, KpiAnalyzer) and kpi_class is not KpiAnalyzer:\\n                    self.kpi_analyzers[name] = kpi_class()\\n            self.log_info(\"Available KPIs: \" + \", \".join(self.kpi_analyzers.keys()))\\n        except Exception as e:\\n            self.log_error(\"Failed to load KPI analyzers: \" + str(e))\\n\\n    def list_kpis(self):\\n        \"\"\"\\n        Returns a list of all available KPI names that can be monitored.\\n        \"\"\"\\n        return list(self.kpi_analyzers.keys())\\n\\n    def enable_kpi(self, kpi_name, periodicity=None, enable_local_storage=False):\\n        \"\"\"\\n        Enable a specific KPI by its name with optional periodicity and local storage settings.\\n        \"\"\"\\n        if kpi_name in self.kpi_analyzers:\\n            kpi_analyzer = self.kpi_analyzers[kpi_name]\\n            kpi_analyzer.enable()\\n            if periodicity:\\n                kpi_analyzer.set_periodicity(periodicity)\\n            kpi_analyzer.enable_local_storage(enable_local_storage)\\n            self.log_info(f\"KPI {kpi_name} enabled with periodicity {periodicity} and local storage {enable_local_storage}\")\\n        else:\\n            self.log_error(f\"KPI {kpi_name} is not available\")\\n\\n    def enable_all_kpis(self):\\n        \"\"\"\\n        Enable monitoring for all identified KPIs.\\n        \"\"\"\\n        for kpi_name in self.kpi_analyzers.keys():\\n            self.enable_kpi(kpi_name)\\n\\n    def local_query_kpi(self, kpi_name, mode=\\'default\\'):\\n        \"\"\"\\n        Query the locally observed KPI values with additional logic based on the mode.\\n        \"\"\"\\n        if kpi_name in self.kpi_analyzers:\\n            kpi_analyzer = self.kpi_analyzers[kpi_name]\\n            result = kpi_analyzer.query(mode)\\n            self.log_info(f\"Local query for KPI {kpi_name} with mode {mode}: {result}\")\\n            return result\\n        else:\\n            self.log_error(f\"KPI {kpi_name} is not available for local querying\")\\n            return None\\n\\n    def remote_query_kpi(self, kpi_name):\\n        \"\"\"\\n        Query KPI data from a remote cloud service with enhanced logging.\\n        \"\"\"\\n        if kpi_name in self.kpi_analyzers:\\n            kpi_analyzer = self.kpi_analyzers[kpi_name]\\n            try:\\n                result = kpi_analyzer.remote_query()\\n                self.log_info(f\"Remote query for KPI {kpi_name}: {result}\")\\n                return result\\n            except Exception as e:\\n                self.log_error(f\"Failed to perform remote query for KPI {kpi_name}: {str(e)}\")\\n                return None\\n        else:\\n            self.log_error(f\"KPI {kpi_name} is not available for remote querying\")\\n            return None\\n\\n    def set_source(self, source):\\n        \"\"\"\\n        Set the trace source. Enable the KPI analyzers to use the provided source.\\n        \"\"\"\\n        Analyzer.set_source(self, source)\\n        for kpi_analyzer in self.kpi_analyzers.values():\\n            kpi_analyzer.set_source(source)',\n",
       "   'expected_output': '#!/usr/bin/python\\n# Filename: kpi_manager_modified.py\\n\"\"\"\\nkpi_manager_modified.py\\nAn unified interface for users to track and query KPIs with modified calculations\\n\\nAuthor: Yuanjie Li\\n\"\"\"\\n\\n__all__ = [\"KPIManagerModified\"]\\n\\nfrom ..analyzer import *\\nimport sys, inspect, os\\n\\n\\nclass KPIManagerModified(Analyzer):\\n\\n    \"\"\"\\n    An unified interface for users to track and query KPIs\\n    \"\"\"\\n\\n    supported_kpis={} # Supported KPIs: kpi_name -> KPIAnalyzer name\\n\\n    def __init__(self):\\n        Analyzer.__init__(self)\\n        self.__check_kpis()\\n\\n\\n\\n    def __check_kpis(self):\\n\\n        \"\"\"\\n        Find and include all supported KPIs into KPIManager.supported_kpis\\n        \"\"\"\\n        module_tmp = __import__(\"mobile_insight\")\\n        for item in inspect.getmembers(module_tmp.analyzer.kpi, inspect.isclass):\\n            if item[1].__bases__[0].__name__ ==  \"KpiAnalyzer\":\\n                tmp_module = item[1]()\\n                for kpi in tmp_module.list_kpis():\\n                        KPIManagerModified.supported_kpis[kpi] = item[0]\\n                        self.log_info(kpi)\\n        \\n\\n    def list_kpis(self):\\n        \"\"\"\\n        Return a list of available KPIs \\n\\n        :returns: a list of string, each of which is a KPI name\\n        \"\"\"\\n        return list(self.supported_kpis.keys())\\n\\n    def enable_all_kpis(self, enable_storage = False):\\n        \"\"\"\\n        Enable all KPIs\\' monitoring\\n        \\n        :param enable_storage: Whether to locally store the kpi. False by default\\n        :type enable_storage: boolean\\n        \"\"\"\\n        for kpi_name in self.list_kpis():\\n            self.enable_kpi(kpi_name, enable_storage)\\n\\n\\n    def enable_kpi(self, kpi_name, periodicity=\\'0s\\', cell=None, enable_storage = True):\\n        \"\"\"\\n        Enable the KPI monitoring with slight modification\\n\\n        :param kpi_name: The KPI to be monitored\\n        :type kpi_name: string\\n        :param enable_storage: Whether to locally store the kpi. False by default\\n        :type enable_storage: boolean\\n        :returns: True if successfully activated, False otherwise\\n        \"\"\"\\n\\n        if kpi_name not in self.supported_kpis:\\n            self.log_warning(\"KPI does not exist: \"+kpi_name)\\n            return False\\n\\n        try: \\n            kpi_analyzer_name = self.supported_kpis[kpi_name]\\n            self.include_analyzer(kpi_analyzer_name, [])\\n            self.get_analyzer(kpi_analyzer_name).enable_local_storage(enable_storage)\\n            self.get_analyzer(kpi_analyzer_name).set_periodicity(kpi_name, periodicity)\\n            self.get_analyzer(kpi_analyzer_name).set_cell(kpi_name, cell)\\n            # Modification: Log additional info for KPI activation\\n            self.log_info(f\"Enable KPI: {kpi_name} with periodicity: {periodicity} and storage: {enable_storage}\")\\n            return True\\n        except Exception as e:\\n            # Import failure\\n            self.log_warning(\"Fail to activate KPI: \"+kpi_name)    \\n            return False\\n\\n\\n    def local_query_kpi(self, kpi_name, mode = \\'cell\\', timestamp = None):\\n        \"\"\"\\n        Query the phone\\'s locally observed KPI\\n\\n        :param kpi_name: The KPI to be queried\\n        :type kpi_name: string\\n        :param timestamp: The timestamp of the KPI. If None, this function returns the latest KPI\\n        :type timestamp: datetime\\n        :returns: The KPI value, or None if the KPI is not available\\n        \"\"\"\\n        if kpi_name not in self.supported_kpis:\\n            self.log_warning(\"KPI does not exist: \"+kpi_name)\\n            return None\\n\\n        kpi_agent = self.get_analyzer(self.supported_kpis[kpi_name])\\n        if not kpi_agent:\\n            # KPI analyzer not triggered\\n            self.log_warning(\"KPI not activated yet: \"+kpi_name)\\n            self.enable_kpi(kpi_name)\\n            return None\\n\\n        # Modification: Adjust query mode logic (e.g., simulate different processing)\\n        if mode == \\'cell\\':\\n            self.log_info(f\"Querying KPI: {kpi_name} in cell mode\")\\n        else:\\n            self.log_info(f\"Querying KPI: {kpi_name} in {mode} mode\")\\n        \\n        return kpi_agent.local_query_kpi(kpi_name, mode, timestamp)\\n\\n    def remote_query_kpi(self, kpi_name, phone_model, operator, gps, timestamp):\\n        \"\"\"\\n        Query the remote cloud for the KPI\\n\\n        :param kpi_name: The KPI to be queried\\n        :type kpi_name: string\\n        :param phone_model: The the phone model\\n        :type phone_model: string\\n        :param operator: The network operator\\n        :type operator: string\\n        :param gps: The GPS coordinate\\n        :type gps: string\\n        :param timestamp: The timestamp of the KPI. \\n        :type timestamp: datetime\\n        :returns: The KPI value, or None if the KPI is not available\\n        \"\"\"\\n        if kpi_name not in KPIManagerModified.supported_kpis:\\n            self.log_warning(\"KPI does not exist: \"+kpi_name)\\n            return None\\n\\n        kpi_agent = self.get_analyzer(KPIManagerModified.supported_kpis[kpi_name])\\n        if not kpi_agent:\\n            # KPI analyzer not triggered\\n            self.log_warning(\"KPI not activated yet: \"+kpi_name)\\n            self.enable_kpi(kpi_name)\\n            return None\\n\\n        # Modification: Log additional information for remote query\\n        self.log_info(f\"Remote query for KPI: {kpi_name} at timestamp: {timestamp}\")\\n        \\n        return kpi_agent.local_query_kpi(kpi_name, phone_model, operator, gps, timestamp)',\n",
       "   'context': [],\n",
       "   'retrieval_context': []}],\n",
       " 'confident_link': None}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_results.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__class_vars__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_pydantic_core_schema__',\n",
       " '__get_pydantic_json_schema__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__pydantic_complete__',\n",
       " '__pydantic_core_schema__',\n",
       " '__pydantic_custom_init__',\n",
       " '__pydantic_decorators__',\n",
       " '__pydantic_extra__',\n",
       " '__pydantic_fields_set__',\n",
       " '__pydantic_generic_metadata__',\n",
       " '__pydantic_init_subclass__',\n",
       " '__pydantic_parent_namespace__',\n",
       " '__pydantic_post_init__',\n",
       " '__pydantic_private__',\n",
       " '__pydantic_root_model__',\n",
       " '__pydantic_serializer__',\n",
       " '__pydantic_validator__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_calculate_keys',\n",
       " '_check_frozen',\n",
       " '_copy_and_set_values',\n",
       " '_get_value',\n",
       " '_iter',\n",
       " 'confident_link',\n",
       " 'construct',\n",
       " 'copy',\n",
       " 'dict',\n",
       " 'from_orm',\n",
       " 'json',\n",
       " 'model_computed_fields',\n",
       " 'model_config',\n",
       " 'model_construct',\n",
       " 'model_copy',\n",
       " 'model_dump',\n",
       " 'model_dump_json',\n",
       " 'model_extra',\n",
       " 'model_fields',\n",
       " 'model_fields_set',\n",
       " 'model_json_schema',\n",
       " 'model_parametrized_name',\n",
       " 'model_post_init',\n",
       " 'model_rebuild',\n",
       " 'model_validate',\n",
       " 'model_validate_json',\n",
       " 'model_validate_strings',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'test_results',\n",
       " 'update_forward_refs',\n",
       " 'validate']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results saved to evaluation_results.json\n"
     ]
    }
   ],
   "source": [
    "# Save results to a JSON file\n",
    "output_file = \"evaluation_results.json\"\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(evaluation_results.dict(), file, indent=4)\n",
    "\n",
    "print(f\"Evaluation results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_dataset_baseline = EvaluationDataset()\n",
    "\n",
    "# Add as test cases\n",
    "outer_dataset_baseline.add_test_cases_from_csv_file(\n",
    "    # file_path is the absolute path to you .csv file\n",
    "    file_path=r\"C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\outer_dataset_baseline.csv\",\n",
    "    input_col_name=\"input\",\n",
    "    actual_output_col_name=\"actual_output\",\n",
    "    expected_output_col_name=\"expected_output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also call the evaluate() function directly\n",
    "evaluation_results = evaluate(test_cases=outer_dataset_baseline, metrics=[hallucination_metric, answer_relevancy_metric, correctness_metric])\n",
    "# run_async=False, max_concurrent=1, throttle_value=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to a JSON file\n",
    "output_file = \"evaluation_results.json\"\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(evaluation_results, file, indent=4)\n",
    "\n",
    "print(f\"Evaluation results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_dataset_ICL = EvaluationDataset()\n",
    "\n",
    "# Add as test cases\n",
    "outer_dataset_baseline.add_test_cases_from_csv_file(\n",
    "    # file_path is the absolute path to you .csv file\n",
    "    file_path=r\"C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\outer_dataset_baseline.csv\",\n",
    "    input_col_name=\"input\",\n",
    "    actual_output_col_name=\"actual_output\",\n",
    "    expected_output_col_name=\"expected_output\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
