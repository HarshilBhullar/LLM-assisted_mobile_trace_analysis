{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPEN_AI_KEY\"] = api_key\n",
    "import subprocess\n",
    "import re\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Run a Python script and capture its output\n",
    "def run_script(file_path):\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"python\", file_path],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        return result.stdout.strip()\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error running script {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Step 2: Evaluate outputs using an LLM\n",
    "def evaluate_outputs(true_output, generated_output, llm):\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"true_output\", \"generated_output\"],\n",
    "        template=\"\"\"\n",
    "The outputs below are generated by running two Python scripts:\n",
    "1. The first output is from the true script (expected result).\n",
    "2. The second output is from the generated script (model prediction).\n",
    "\n",
    "True Output:\n",
    "{true_output}\n",
    "\n",
    "Generated Output:\n",
    "{generated_output}\n",
    "\n",
    "Compare these outputs semantically. Do they represent equivalent results, even if formatting or naming conventions differ? \\\n",
    "    If not, explain the differences. Focus on the metric values and the core outputs of the scripts. \\\n",
    "    Respond with a detailed evaluation and include a binary decision (\"0\" for equivalent and \"1\" for not equivalent).\n",
    "\n",
    "Decision: 0 (equivalent) or 1 (not equivalent)\n",
    "\"\"\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    inputs = {\n",
    "        \"true_output\": true_output,\n",
    "        \"generated_output\": generated_output,\n",
    "    }\n",
    "    result = chain.invoke(inputs)\n",
    "    llm_output = result['text'].strip()\n",
    "\n",
    "    match = re.search(r\"Decision:\\s*([01])\", llm_output)\n",
    "    if match:\n",
    "        decision = int(match.group(1))  # Convert the matched value to an integer\n",
    "        equivalent = decision == 0     # True if 0, False if 1\n",
    "    else:\n",
    "        print(\"Error parsing LLM decision. Defaulting to Not Equivalent.\")\n",
    "        decision = 1\n",
    "        equivalent = False\n",
    "\n",
    "    return llm_output, equivalent\n",
    "\n",
    "# Step 3: Filter file names based on inclusion rules\n",
    "def filter_files(file_list, base_folder, generated_folder):\n",
    "    filtered_files = []\n",
    "    for entry in file_list:\n",
    "        file_name = entry.split()[0]\n",
    "        numbers = entry.split()[1:] if len(entry.split()) > 1 else [\"1\", \"2\", \"3\"]\n",
    "\n",
    "        for num in numbers:\n",
    "            filtered_files.append({\n",
    "                \"true_file\": os.path.join(base_folder, f\"modified_{file_name}_{num}.py\"),\n",
    "                \"generated_file\": os.path.join(generated_folder, f\"prompt_{file_name}_{num}_generated_ICL.py\"),\n",
    "                \"file_name\": f\"{file_name}_{num}\"\n",
    "            })\n",
    "\n",
    "    return filtered_files\n",
    "\n",
    "# Step 4: Compare true and generated files\n",
    "def compare_outputs(file_entries, llm):\n",
    "    comparison_results = {}\n",
    "\n",
    "    for entry in file_entries:\n",
    "        true_file_path = entry[\"true_file\"]\n",
    "        generated_file_path = entry[\"generated_file\"]\n",
    "        file_name = entry[\"file_name\"]\n",
    "\n",
    "        if not os.path.exists(true_file_path):\n",
    "            print(f\"True file not found: {true_file_path}\")\n",
    "            continue\n",
    "\n",
    "        if not os.path.exists(generated_file_path):\n",
    "            print(f\"Generated file not found: {generated_file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Run both scripts and capture outputs\n",
    "        true_output = run_script(true_file_path)\n",
    "        generated_output = run_script(generated_file_path)\n",
    "\n",
    "        if true_output is None or generated_output is None:\n",
    "            print(f\"Error running one of the scripts: {file_name}\")\n",
    "            continue\n",
    "\n",
    "        # Evaluate outputs using the LLM\n",
    "        llm_output, equivalent = evaluate_outputs(true_output, generated_output, llm)\n",
    "\n",
    "        # Store the results\n",
    "        comparison_results[file_name] = {\n",
    "            \"true_output\": true_output,\n",
    "            \"generated_output\": generated_output,\n",
    "            \"evaluation\": llm_output,\n",
    "            \"equivalent\": equivalent\n",
    "        }\n",
    "\n",
    "        print(f\"Comparison for {file_name} completed.\")\n",
    "\n",
    "    return comparison_results\n",
    "\n",
    "# Step 5: Save results to a file\n",
    "def save_results(results, output_file):\n",
    "    import json\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # File list provided in the request\n",
    "    file_list = [\n",
    "        \"dl_retx_analyzer_test.py\",\n",
    "        \"lte_dl_retx_analyzer_example.py\",\n",
    "        \"lte_mac_analyzer_example.py 1 2\",\n",
    "        \"lte_meas_analyzer_example.py\",\n",
    "        \"lte_phy_analyzer_example.py 1\",\n",
    "        \"lte_rlc_analyzer_example.py 1 2\",\n",
    "        \"mm_analyzer_example.py 1 3\",\n",
    "        \"modem_debug_analyzer_example.py 1\",\n",
    "        \"nn_rrc_analyzer_example.py 1 2\",\n",
    "        \"offline-latency-analysis-ul.py\",\n",
    "        \"track_cell_info_analyzer_example.py 2 3\",\n",
    "        \"ul_mac_latency_analyzer_example.py 1 2\",\n",
    "        \"umts_nas_analyzer_example.py 1\",\n",
    "        \"wcdma_rrc_analyzer_example.py 1 2\"\n",
    "    ]\n",
    "\n",
    "    # Base folder containing the files\n",
    "    base_folder = '/home/harshbull/Desktop/LLM-assisted_mobile_trace_analysis/generated_outer_working_dataset'\n",
    "\n",
    "    generated_folder = '/home/harshbull/Desktop/LLM-assisted_mobile_trace_analysis/generated_outer_analyzers_ICL'\n",
    "\n",
    "    # Filter the files based on the rules\n",
    "    filtered_files = filter_files(file_list, base_folder, generated_folder)\n",
    "\n",
    "    # Initialize the LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "    # Compare outputs\n",
    "    results = compare_outputs(filtered_files, llm)\n",
    "\n",
    "    # Save results\n",
    "    save_results(results, r\"comparison_results_ICL.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobile",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
