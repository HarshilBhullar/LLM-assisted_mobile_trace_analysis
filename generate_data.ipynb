{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPEN_AI_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FULLY GENERATED ANALYZER SCRIPT\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "# Step 1: Load the code files from the repository\n",
    "loader = DirectoryLoader(\n",
    "    path=r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\mobile_insight',\n",
    "    glob='**/*.py',             # Only .py files\n",
    "    exclude=['**/__pycache__/**', '**/*.pyc']  # Exclude cache and .pyc files\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "# Step 2: Split the code files into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    separators=['\\n\\n', '\\n', ' ', '']\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Step 3: Create embeddings for the chunks\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Step 4: Store the embeddings in a vectorstore\n",
    "vectorstore = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "def get_random_context(k=5):\n",
    "    # Randomly select k documents from the list of texts\n",
    "    selected_docs = random.sample(texts, min(k, len(texts)))\n",
    "    context = '\\n\\n'.join([doc.page_content for doc in selected_docs])\n",
    "    return context\n",
    "\n",
    "# Step 6: Read the two example files\n",
    "with open(r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\testcases\\attach_stats\\attach_stats.py', 'r') as f:\n",
    "    example1 = f.read()\n",
    "\n",
    "with open(r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\testcases\\interval_stats\\interval_stats.py', 'r') as f:\n",
    "    example2 = f.read()\n",
    "\n",
    "# Step 7: Create the prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['example1', 'example2', 'context'],\n",
    "    template=\"\"\"\n",
    "You are an AI assistant that generates code for analyzers using the given Python library. Here are two examples of analyzer code:\n",
    "\n",
    "Example 1:\n",
    "{example1}\n",
    "\n",
    "Example 2:\n",
    "{example2}\n",
    "\n",
    "Using the above examples as a guide, please generate a new analyzer code that is different from the examples provided. \n",
    "The new analyzer should demonstrate a unique functionality or feature of the library.\n",
    "NOTE: ONLY PROVIDE PYTHON CODE, DO NOT ADD ANY OTHER TEXT BEFORE OR AFTER AS THIS OUTPUT IS BEING SAVED DIRECTLY INTO A PY FILE.\n",
    "\n",
    "You have access to the following relevant code snippets from the library:\n",
    "{context}\n",
    "\n",
    "Please make sure to properly use the library's functions and classes as per the context provided.\n",
    "\n",
    "Generated Code:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Step 8: Initialize the LLM\n",
    "llm = ChatOpenAI(model_name='gpt-4o', temperature=0.7)\n",
    "\n",
    "\n",
    "# Step 9: Define the function to generate code examples\n",
    "def generate_code_examples(num_examples):\n",
    "    generated_examples = []\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    \n",
    "    for _ in range(num_examples):\n",
    "        # Retrieve random context documents\n",
    "        context = get_random_context(k=5)\n",
    "        \n",
    "        # Prepare the inputs to the prompt\n",
    "        inputs = {\n",
    "            'example1': example1,\n",
    "            'example2': example2,\n",
    "            'context': context\n",
    "        }\n",
    "        \n",
    "        # Run the chain to generate code\n",
    "        result = chain.invoke(inputs)\n",
    "        \n",
    "        # Append the generated code to the list\n",
    "        generated_code = result['text'].strip()\n",
    "        \n",
    "        # Append the generated code to the list\n",
    "        generated_examples.append(generated_code)\n",
    "        \n",
    "    return generated_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Code 4 saved to generated_analyzer_24.py\n",
      "Generated Code 5 saved to generated_analyzer_25.py\n",
      "Generated Code 6 saved to generated_analyzer_26.py\n",
      "Generated Code 7 saved to generated_analyzer_27.py\n",
      "Generated Code 8 saved to generated_analyzer_28.py\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Generate multiple code examples\n",
    "num_examples_to_generate = 5  # Adjust the number as needed\n",
    "generated_codes = generate_code_examples(num_examples_to_generate)\n",
    "\n",
    "# Step 11: Save the generated codes to files or print them\n",
    "for idx, code in enumerate(generated_codes):\n",
    "    filename = f'generated_analyzer_{23+idx+1}.py'\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(code)\n",
    "    print(f\"Generated Code {3+idx+1} saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic dataset created and saved.\n"
     ]
    }
   ],
   "source": [
    "### GENERATED PROMPTS ANALYZER PAIRS SCRIPT\n",
    "\n",
    "import json\n",
    "\n",
    "# Step 1: Load real prompt-code pairs from folders\n",
    "def load_prompt_code_pairs(directory):\n",
    "    prompt_code_pairs = []\n",
    "    \n",
    "    # Each subdirectory contains a prompt (txt) and code (py) file\n",
    "    for folder_name in os.listdir(directory):\n",
    "        if folder_name == 'logs':\n",
    "            continue\n",
    "        folder_path = os.path.join(directory, folder_name)\n",
    "        \n",
    "        if os.path.isdir(folder_path):\n",
    "            # Find the txt and py files\n",
    "            prompt_file = next((f for f in os.listdir(folder_path) if f.endswith('.txt')), None)\n",
    "            code_file = next((f for f in os.listdir(folder_path) if f.endswith('.py')), None)\n",
    "            \n",
    "            if prompt_file and code_file:\n",
    "                # Load the prompt\n",
    "                with open(os.path.join(folder_path, prompt_file), 'r') as f:\n",
    "                    prompt = f.read().strip()\n",
    "                \n",
    "                # Load the code\n",
    "                with open(os.path.join(folder_path, code_file), 'r') as f:\n",
    "                    code = f.read().strip()\n",
    "                \n",
    "                # Append to the list as a dict\n",
    "                prompt_code_pairs.append({\n",
    "                    'prompt': prompt,\n",
    "                    'code': code\n",
    "                })\n",
    "    \n",
    "    return prompt_code_pairs\n",
    "\n",
    "# Load real prompt-code pairs\n",
    "real_examples_directory = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\testcases'\n",
    "real_prompt_code_pairs = load_prompt_code_pairs(real_examples_directory)\n",
    "\n",
    "# Step 2: Create the prompt template for generating new prompts based on Python code\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"example1_prompt\", \"example1_code\", \"example2_prompt\", \"example2_code\", \"generated_code\"],\n",
    "    template=\"\"\"\n",
    "Below are two examples of prompt-code pairs:\n",
    "\n",
    "Example 1:\n",
    "Prompt: {example1_prompt}\n",
    "Code:\n",
    "{example1_code}\n",
    "\n",
    "Example 2:\n",
    "Prompt: {example2_prompt}\n",
    "Code:\n",
    "{example2_code}\n",
    "\n",
    "Given the code below, generate a prompt for it following the structure of the 2 examples:\n",
    "\n",
    "Code:\n",
    "{generated_code}\n",
    "\n",
    "Generated Prompt:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Step 3: Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
    "\n",
    "# Step 4: Function to generate prompts based on Python code\n",
    "def generate_prompts_for_code(generated_code_folder, real_examples):\n",
    "    generated_prompts = {}\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Iterate over all .py files in the generated code folder\n",
    "    for py_file in os.listdir(generated_code_folder):\n",
    "        if py_file.endswith('.py'):\n",
    "            file_path = os.path.join(generated_code_folder, py_file)\n",
    "\n",
    "            # Load the generated Python code\n",
    "            with open(file_path, 'r') as f:\n",
    "                generated_code = f.read()\n",
    "\n",
    "            # Use the first two real examples as in-context examples\n",
    "            example1 = real_examples[0]\n",
    "            example2 = real_examples[1]\n",
    "\n",
    "            # Prepare the inputs for the LLM\n",
    "            inputs = {\n",
    "                \"example1_prompt\": example1['prompt'],\n",
    "                \"example1_code\": example1['code'],\n",
    "                \"example2_prompt\": example2['prompt'],\n",
    "                \"example2_code\": example2['code'],\n",
    "                \"generated_code\": generated_code\n",
    "            }\n",
    "\n",
    "            # Generate the prompt for the current Python file\n",
    "            result = chain.invoke(inputs)\n",
    "            generated_prompt = result['text'].strip()\n",
    "\n",
    "            # Store the generated prompt and the corresponding Python code in the dictionary\n",
    "            generated_prompts[generated_prompt] = generated_code\n",
    "\n",
    "    return generated_prompts\n",
    "\n",
    "# Step 5: Define the folder where all generated Python files are saved\n",
    "generated_code_folder = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\generated_dataset'\n",
    "\n",
    "# Step 6: Generate the prompts for the Python files\n",
    "generated_dataset = generate_prompts_for_code(generated_code_folder, real_prompt_code_pairs)\n",
    "\n",
    "# Step 7: Save the generated dataset (optional, saving as JSON)\n",
    "with open('synthetic_dataset.json', 'w') as f:\n",
    "    json.dump(generated_dataset, f, indent=4)\n",
    "\n",
    "print(\"Synthetic dataset created and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODIFIED GENERATION ANALYZER SCRIPT\n",
    "\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# Step 1: Load and chunk the codebase, then create embeddings\n",
    "def index_codebase(directory):\n",
    "    # Load all .py files from the directory\n",
    "    loader = DirectoryLoader(\n",
    "        path=directory,\n",
    "        glob=\"**/*.py\",  # Only .py files\n",
    "        exclude=[\"**/__pycache__/**\", \"**/*.pyc\"]\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        separators=['\\n\\n', '\\n', ' ', '']\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # Create embeddings for each chunk\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = Chroma.from_documents(chunks, embeddings)\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "# Index the codebase\n",
    "codebase_directory = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\mobile_insight'\n",
    "vectorstore = index_codebase(codebase_directory)\n",
    "\n",
    "# Step 2: Load a real example analyzer code\n",
    "def load_real_analyzer(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        real_analyzer_code = f.read()\n",
    "    return real_analyzer_code\n",
    "\n",
    "# Load one example real analyzer code\n",
    "real_analyzer_path = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\testcases\\attach_stats\\attach_stats.py'  # Update with the actual path to your real analyzer file\n",
    "real_analyzer_code = load_real_analyzer(real_analyzer_path)\n",
    "\n",
    "# Step 3: Create the prompt template for modifying the real analyzer code\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"real_analyzer_code\", \"retrieved_context\"],\n",
    "    template=\"\"\"\n",
    "You are writing modified example python files that use an open-source project library called Mobileinsight.\n",
    "\n",
    "Below are relevant parts of the Python codebase that provide useful context:\n",
    "\n",
    "{retrieved_context}\n",
    "\n",
    "Here is an example of an existing analyzer Python file from the codebase:\n",
    "\n",
    "Real Analyzer Code:\n",
    "{real_analyzer_code}\n",
    "\n",
    "Using the codebase context and the real analyzer code as a reference, create a slightly modified version of the analyzer. \n",
    "    The new analyzer should perform a similar analysis but with some changes, such as adjusting metrics, altering data processing,\n",
    "    or applying a different calculation. Ensure the modified analyzer remains functional and consistent with the codebase's style and structure.\n",
    "    You do NOT need to make any drastic changes;  adding some slightly altered output through modified calculations should be enough.\n",
    "\n",
    "NOTE: ONLY PROVIDE PYTHON CODE, DO NOT ADD ANY OTHER TEXT BEFORE OR AFTER AS THIS OUTPUT IS BEING SAVED DIRECTLY INTO A PY FILE.\n",
    "\n",
    "Generated Modified Analyzer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Step 4: Initialize the LLM\n",
    "llm = ChatOpenAI(model_name='gpt-4o', temperature=0.7)\n",
    "\n",
    "def generate_modified_analyzers(num_examples, vectorstore, real_analyzer_code):\n",
    "    modified_analyzers = []\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    \n",
    "    for _ in range(num_examples):\n",
    "        # Retrieve relevant code snippets\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "        relevant_docs = retriever.get_relevant_documents(real_analyzer_code)\n",
    "        retrieved_context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "        \n",
    "        # Prepare the inputs for the LLM\n",
    "        inputs = {\n",
    "            \"real_analyzer_code\": real_analyzer_code,\n",
    "            \"retrieved_context\": retrieved_context\n",
    "        }\n",
    "\n",
    "        # Generate the modified analyzer code\n",
    "        result = chain.invoke(inputs)\n",
    "        modified_analyzer_code = result['text'].strip()\n",
    "\n",
    "        # Append the modified analyzer code to the list\n",
    "        modified_analyzers.append(modified_analyzer_code)\n",
    "\n",
    "    return modified_analyzers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Modified analyzer 85 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_85.py\n",
      "Modified analyzer 86 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_86.py\n",
      "Modified analyzer 87 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_87.py\n",
      "Modified analyzer 88 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_88.py\n",
      "Modified analyzer 89 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_89.py\n",
      "Modified analyzer 90 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_90.py\n",
      "Modified analyzer 91 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_91.py\n",
      "Modified analyzer 92 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_92.py\n",
      "Modified analyzer 93 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_93.py\n",
      "Modified analyzer 94 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_94.py\n",
      "Modified analyzer 95 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_95.py\n",
      "Modified analyzer 96 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_96.py\n",
      "Modified analyzer 97 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_97.py\n",
      "Modified analyzer 98 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_98.py\n",
      "Modified analyzer 99 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_99.py\n",
      "Modified analyzer 100 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_100.py\n",
      "All modified analyzers generated and saved.\n"
     ]
    }
   ],
   "source": [
    "num_examples_to_generate = 2  # Adjust as needed\n",
    "real_examples_folder = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\mobile_insight\\examples'\n",
    "modified_analyzers = []\n",
    "i=0\n",
    "for py_file in os.listdir(real_examples_folder):\n",
    "    if py_file.endswith('.py'):\n",
    "        file_path = os.path.join(real_examples_folder, py_file)\n",
    "        with open(file_path, 'r') as f:\n",
    "                real_analyzer_code = f.read()\n",
    "        modified_analyzers += generate_modified_analyzers(num_examples_to_generate, vectorstore, real_analyzer_code)\n",
    "        print(i)\n",
    "        i +=1\n",
    "        if i == 8:\n",
    "             break\n",
    "\n",
    "# Step 7: Save the modified analyzers to files (optional)\n",
    "output_directory = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "for idx, code in enumerate(modified_analyzers):\n",
    "    filename = os.path.join(output_directory, f'modified_analyzer_{idx + 84 + 1}.py')\n",
    "    code = code.replace(\"```python\", \"\", 1)\n",
    "    code = code.replace(\"```\", \"\", 1)\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(code)\n",
    "    print(f\"Modified analyzer {idx + 84 + 1} saved to {filename}\")\n",
    "\n",
    "print(\"All modified analyzers generated and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified synthetic dataset created and saved.\n"
     ]
    }
   ],
   "source": [
    "### MODIFIED GENERATION PROMPTS ANALYZER PAIRS SCRIPT\n",
    "\n",
    "import json\n",
    "\n",
    "# Step 1: Load real prompt-code pairs from folders\n",
    "def load_prompt_code_pairs(directory):\n",
    "    prompt_code_pairs = []\n",
    "    \n",
    "    # Each subdirectory contains a prompt (txt) and code (py) file\n",
    "    for folder_name in os.listdir(directory):\n",
    "        if folder_name == 'logs':\n",
    "            continue\n",
    "        folder_path = os.path.join(directory, folder_name)\n",
    "        \n",
    "        if os.path.isdir(folder_path):\n",
    "            # Find the txt and py files\n",
    "            prompt_file = next((f for f in os.listdir(folder_path) if f.endswith('.txt')), None)\n",
    "            code_file = next((f for f in os.listdir(folder_path) if f.endswith('.py')), None)\n",
    "            \n",
    "            if prompt_file and code_file:\n",
    "                # Load the prompt\n",
    "                with open(os.path.join(folder_path, prompt_file), 'r') as f:\n",
    "                    prompt = f.read().strip()\n",
    "                \n",
    "                # Load the code\n",
    "                with open(os.path.join(folder_path, code_file), 'r') as f:\n",
    "                    code = f.read().strip()\n",
    "                \n",
    "                # Append to the list as a dict\n",
    "                prompt_code_pairs.append({\n",
    "                    'prompt': prompt,\n",
    "                    'code': code\n",
    "                })\n",
    "    \n",
    "    return prompt_code_pairs\n",
    "\n",
    "# Load real prompt-code pairs\n",
    "real_examples_directory = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\testcases'\n",
    "real_prompt_code_pairs = load_prompt_code_pairs(real_examples_directory)\n",
    "\n",
    "# Step 2: Create the prompt template for generating new prompts based on Python code\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"example1_prompt\", \"example1_code\", \"example2_prompt\", \"example2_code\", \"generated_code\"],\n",
    "    template=\"\"\"\n",
    "Below are two examples of prompt-code pairs:\n",
    "\n",
    "Example 1:\n",
    "Prompt: {example1_prompt}\n",
    "Code:\n",
    "{example1_code}\n",
    "\n",
    "Example 2:\n",
    "Prompt: {example2_prompt}\n",
    "Code:\n",
    "{example2_code}\n",
    "\n",
    "Given the code below, generate a prompt for it following the structure of the 2 examples:\n",
    "\n",
    "Code:\n",
    "{generated_code}\n",
    "\n",
    "Generated Prompt:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Step 3: Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
    "\n",
    "# Step 4: Function to generate prompts based on Python code\n",
    "def generate_prompts_for_code(generated_code_folder, real_examples):\n",
    "    generated_prompts = {}\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Iterate over all .py files in the generated code folder\n",
    "    for py_file in os.listdir(generated_code_folder):\n",
    "        if py_file.endswith('.py'):\n",
    "            file_path = os.path.join(generated_code_folder, py_file)\n",
    "\n",
    "            # Load the generated Python code\n",
    "            with open(file_path, 'r') as f:\n",
    "                generated_code = f.read()\n",
    "\n",
    "            # Use the first two real examples as in-context examples\n",
    "            example1 = real_examples[0]\n",
    "            example2 = real_examples[1]\n",
    "\n",
    "            # Prepare the inputs for the LLM\n",
    "            inputs = {\n",
    "                \"example1_prompt\": example1['prompt'],\n",
    "                \"example1_code\": example1['code'],\n",
    "                \"example2_prompt\": example2['prompt'],\n",
    "                \"example2_code\": example2['code'],\n",
    "                \"generated_code\": generated_code\n",
    "            }\n",
    "\n",
    "            # Generate the prompt for the current Python file\n",
    "            result = chain.invoke(inputs)\n",
    "            generated_prompt = result['text'].strip()\n",
    "\n",
    "            # Store the generated prompt and the corresponding Python code in the dictionary\n",
    "            generated_prompts[generated_prompt] = generated_code\n",
    "\n",
    "    return generated_prompts\n",
    "\n",
    "# Step 5: Define the folder where all generated Python files are saved\n",
    "generated_code_folder = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset'\n",
    "\n",
    "# Step 6: Generate the prompts for the Python files\n",
    "generated_dataset = generate_prompts_for_code(generated_code_folder, real_prompt_code_pairs)\n",
    "\n",
    "# Step 7: Save the generated dataset (optional, saving as JSON)\n",
    "with open('modified_synthetic_dataset.json', 'w') as f:\n",
    "    json.dump(generated_dataset, f, indent=4)\n",
    "\n",
    "print(\"Modified synthetic dataset created and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODIFIED GENERATION OUTER ANALYZER SCRIPT\n",
    "\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# Step 1: Load and chunk the codebase, then create embeddings\n",
    "def index_codebase(directory):\n",
    "    # Load all .py files from the directory\n",
    "    loader = DirectoryLoader(\n",
    "        path=directory,\n",
    "        glob=\"**/*.py\",  # Only .py files\n",
    "        exclude=[\"**/__pycache__/**\", \"**/*.pyc\"]\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        separators=['\\n\\n', '\\n', ' ', '']\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # Create embeddings for each chunk\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = Chroma.from_documents(chunks, embeddings)\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "# Index the codebase\n",
    "codebase_directory = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\mobile_insight'\n",
    "vectorstore = index_codebase(codebase_directory)\n",
    "\n",
    "# Step 2: Load a real example analyzer code\n",
    "def load_real_analyzer(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        real_analyzer_code = f.read()\n",
    "    return real_analyzer_code\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Create the prompt template for modifying the real analyzer code\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"real_outer_analyzer_code\", \"real_inner_analyzer_code\", \"retrieved_context\"],\n",
    "    template=\"\"\"\n",
    "You are writing modified example python files that use an open-source project library called Mobileinsight.\n",
    "There are 2 analyzer files: an outer analyzer file and inner analyzer file. The inner analyzer file uses the Mobileinsight library and Analyzer class definitions to create \\\n",
    "a custom Analyzer class, and the outer analyzer file uses this custom Analyzer class contained in the inner analyzer file to write a script that will evaluate some metrics.\n",
    "You will be given both the outer analyzer file and the inner analyzer file being used by that outer analyzer file, and you will need to use the same inner analyzer file to \\\n",
    "create a slightly modified outer analyzer file. The new analyzer should perform a similar analysis but with some changes, such as adjusting metrics, altering data processing,\n",
    "or applying a different calculation. Ensure the modified analyzer remains functional and consistent with the codebase's style and structure.\n",
    "You do NOT need to make any drastic changes;  adding some slightly altered output through modified calculations should be enough.\n",
    "\n",
    "Below are relevant parts of the Python codebase that provide useful context:\n",
    "\n",
    "{retrieved_context}\n",
    "\n",
    "Here is the example of an existing outer analyzer file from the codebase:\n",
    "\n",
    "Real Outer Analyzer Code:\n",
    "{real_outer_analyzer_code}\n",
    "\n",
    "Here is the example of an existing inner analyzer Python from the codebase:\n",
    "\n",
    "Real Inner Analyzer Code:\n",
    "{real_inner_analyzer_code}\n",
    "\n",
    "NOTE: ONLY PROVIDE PYTHON CODE, DO NOT ADD ANY OTHER TEXT BEFORE OR AFTER AS THIS OUTPUT IS BEING SAVED DIRECTLY INTO A PY FILE.\n",
    "\n",
    "Generated Modified Outer Analyzer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Step 4: Initialize the LLM\n",
    "llm = ChatOpenAI(model_name='gpt-4o', temperature=0.7)\n",
    "\n",
    "def generate_modified_outer_analyzers(num_examples, vectorstore, real_outer_analyzer_code, real_inner_analyzer_code):\n",
    "    modified_analyzers = []\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    \n",
    "    for _ in range(num_examples):\n",
    "        # Retrieve relevant code snippets\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "        relevant_docs = retriever.get_relevant_documents(real_outer_analyzer_code+real_inner_analyzer_code)\n",
    "        retrieved_context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "        \n",
    "        # Prepare the inputs for the LLM\n",
    "        inputs = {\n",
    "            \"real_outer_analyzer_code\": real_outer_analyzer_code,\n",
    "            \"real_inner_analyzer_code\": real_inner_analyzer_code,\n",
    "            \"retrieved_context\": retrieved_context\n",
    "        }\n",
    "\n",
    "        # Generate the modified analyzer code\n",
    "        result = chain.invoke(inputs)\n",
    "        modified_analyzer_code = result['text'].strip()\n",
    "\n",
    "        # Append the modified analyzer code to the list\n",
    "        modified_analyzers.append(modified_analyzer_code)\n",
    "\n",
    "    return modified_analyzers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified analyzer saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\modified_dl_retx_analyzer_test_1.py\n",
      "Modified analyzer saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\modified_dl_retx_analyzer_test_2.py\n",
      "Modified analyzer saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\modified_kpi-manager-test-experimental_1.py\n",
      "Modified analyzer saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\modified_kpi-manager-test-experimental_2.py\n",
      "Modified analyzer saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\modified_kpi-manager-test_1.py\n",
      "Modified analyzer saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\modified_kpi-manager-test_2.py\n",
      "Modified analyzer saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\modified_lte-measurement-example_1.py\n",
      "Modified analyzer saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\modified_lte-measurement-example_2.py\n",
      "Modified analyzer saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\modified_lte-nas-layer-example_1.py\n",
      "Modified analyzer saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\modified_lte-nas-layer-example_2.py\n",
      "Modified analyzer saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\modified_monitor-example_1.py\n",
      "Modified analyzer saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\modified_monitor-example_2.py\n",
      "Modified analyzer saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\modified_msg-statistics-example_1.py\n",
      "Modified analyzer saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\modified_msg-statistics-example_2.py\n",
      "Modified analyzer saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\modified_offline-latency-analysis-ul_1.py\n",
      "Modified analyzer saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\modified_offline-latency-analysis-ul_2.py\n",
      "Modified analyzer saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\modified_online-analysis-example_1.py\n",
      "Modified analyzer saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\modified_online-analysis-example_2.py\n",
      "All modified analyzers generated and saved.\n"
     ]
    }
   ],
   "source": [
    "## GENERATE ALL OUTER ANALYZERS\n",
    "\n",
    "analyzer_files = {\n",
    "    \"lte_dl_retx_analyzer.py\": [\"dl_retx_analyzer_test.py\"],\n",
    "    \"kpi/kpi_manager.py\": [\"kpi-manager-test-experimental.py\", \"kpi-manager-test.py\"],\n",
    "    \"lte_measurement_analyzer.py\": [\"lte-measurement-example.py\"],\n",
    "    \"lte_nas_analyzer.py\": [\"lte-nas-layer-example.py\"],\n",
    "    \"msg_logger.py\": [\"monitor-example.py\"],\n",
    "    \"msg_statistics.py\": [\"msg-statistics-example.py\"],\n",
    "    \"uplink_latency_analyzer.py\": [\"offline-latency-analysis-ul.py\"],\n",
    "    \"nr_rrc_analyzer.py\": [\"online-analysis-example.py\"]\n",
    "}\n",
    "\n",
    "# Define paths for analyzers\n",
    "real_examples_folder = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\mobile_insight\\examples'\n",
    "real_inner_analyzer_folder = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\mobile_insight\\analyzer'\n",
    "output_directory = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Step 6: Loop through the dictionary and generate modified outer analyzers\n",
    "for inner_analyzer, outer_analyzers in analyzer_files.items():\n",
    "    # Load the inner analyzer code\n",
    "    inner_analyzer_path = os.path.join(real_inner_analyzer_folder, inner_analyzer)\n",
    "    real_inner_analyzer_code = load_real_analyzer(inner_analyzer_path)\n",
    "\n",
    "    # Generate modified outer analyzers for each outer analyzer associated with this inner analyzer\n",
    "    for outer_analyzer in outer_analyzers:\n",
    "        outer_analyzer_path = os.path.join(real_examples_folder, outer_analyzer)\n",
    "        real_outer_analyzer_code = load_real_analyzer(outer_analyzer_path)\n",
    "\n",
    "        # Generate modified code\n",
    "        modified_codes = generate_modified_outer_analyzers(2, vectorstore, real_outer_analyzer_code, real_inner_analyzer_code)\n",
    "\n",
    "        # Save each modified analyzer\n",
    "        for idx, modified_code in enumerate(modified_codes):\n",
    "            filename = f\"modified_{os.path.splitext(outer_analyzer)[0]}_{idx + 1}.py\"\n",
    "            output_path = os.path.join(output_directory, filename)\n",
    "            code = modified_code.replace(\"```python\", \"\", 1)\n",
    "            code = code.replace(\"```\", \"\", 1)\n",
    "            with open(output_path, 'w') as f:\n",
    "                f.write(code)\n",
    "            print(f\"Modified analyzer saved to {output_path}\")\n",
    "\n",
    "print(\"All modified analyzers generated and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one example real analyzer code\n",
    "real_outer_analyzer_path = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\mobile_insight\\examples\\offline-latency-analysis-ul.py'\n",
    "real_outer_analyzer_code = load_real_analyzer(real_outer_analyzer_path)\n",
    "\n",
    "real_inner_analyzer_path = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\mobile_insight\\analyzer\\uplink_latency_analyzer.py'\n",
    "real_inner_analyzer_code = load_real_analyzer(real_inner_analyzer_path)\n",
    "\n",
    "msg_stat_ex = generate_modified_outer_analyzers(1, vectorstore, real_outer_analyzer_code, real_inner_analyzer_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\generated_datasets\\outer_generated_dataset'\n",
    "filename = os.path.join(output_directory, f'outer_analyzer_{2}.py')\n",
    "code = msg_stat_ex[0].replace(\"```python\", \"\", 1)\n",
    "code = code.replace(\"```\", \"\", 1)\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(code)\n",
    "print(f\"Modified analyzer {1} saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SINGLE MODIFIED OUTER GENERATION PROMPTS ANALYZER PAIRS SCRIPT\n",
    "\n",
    "import json\n",
    "\n",
    "# Step 1: Load real prompt-code pairs from folders\n",
    "def load_prompt_code_pairs(directory):\n",
    "    prompt_code_pairs = []\n",
    "    \n",
    "    # Each subdirectory contains a prompt (txt) and code (py) file\n",
    "    for folder_name in os.listdir(directory):\n",
    "        if folder_name == 'logs':\n",
    "            continue\n",
    "        folder_path = os.path.join(directory, folder_name)\n",
    "        \n",
    "        if os.path.isdir(folder_path):\n",
    "            # Find the txt and py files\n",
    "            prompt_file = next((f for f in os.listdir(folder_path) if f.endswith('.txt')), None)\n",
    "            code_file = next((f for f in os.listdir(folder_path) if f.endswith('.py')), None)\n",
    "            \n",
    "            if prompt_file and code_file:\n",
    "                # Load the prompt\n",
    "                with open(os.path.join(folder_path, prompt_file), 'r') as f:\n",
    "                    prompt = f.read().strip()\n",
    "                \n",
    "                # Load the code\n",
    "                with open(os.path.join(folder_path, code_file), 'r') as f:\n",
    "                    code = f.read().strip()\n",
    "                \n",
    "                # Append to the list as a dict\n",
    "                prompt_code_pairs.append({\n",
    "                    'prompt': prompt,\n",
    "                    'code': code\n",
    "                })\n",
    "    \n",
    "    return prompt_code_pairs\n",
    "\n",
    "# Load real prompt-code pairs\n",
    "real_examples_directory = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\testcases'\n",
    "real_prompt_code_pairs = load_prompt_code_pairs(real_examples_directory)\n",
    "\n",
    "# Step 2: Create the prompt template for generating new prompts based on Python code\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"example1_prompt\", \"example1_code\", \"example2_prompt\", \"example2_code\", \"inner_analyzer_code\", \"generated_code\"],\n",
    "    template=\"\"\"\n",
    "Below are two examples of prompt-code pairs:\n",
    "\n",
    "Example 1:\n",
    "Prompt: {example1_prompt}\n",
    "Code:\n",
    "{example1_code}\n",
    "\n",
    "Example 2:\n",
    "Prompt: {example2_prompt}\n",
    "Code:\n",
    "{example2_code}\n",
    "\n",
    "The prompts are used by an LLM to generate the code. However, I have a different need. \n",
    "There are 2 analyzer files: an outer analyzer file and inner analyzer file. The inner analyzer file uses the Mobileinsight library and Analyzer class definitions to create \\\n",
    "a custom Analyzer class, and the outer analyzer file uses this custom Analyzer class contained in the inner analyzer file to write a script that will evaluate some metrics.\n",
    "Instead of having a prompt like the above 2 example prompts that don't give any analyzer file (inner or outer) and only provides instructions on the new analyzer file, \\\n",
    "I need a prompt that can give the code for an inner analyzer file, and instructions on what is required for an outer analyzer file. This prompt will then be used to generate \\\n",
    "a new outer analyzer file.\n",
    "\n",
    "Your task is the generate a prompt for the given outer analyzer file below.\\\n",
    "I will be giving you the outer analyzer file, as well as the inner analyzer file that was used to make it, and your task is to create a prompt with instructions on what the \\\n",
    "outer analyzer file should do, noting that it should be using the inner analyzer file and following a similar tyle style to the 2 examples above. \\\n",
    "You don't need to provide the code for the inner analyzer file; I will be taking your output and appending the inner analyzer file code to it.\n",
    "\n",
    "Inner analyzer code:\n",
    "{inner_analyzer_code}\n",
    "\n",
    "Outer analyzer code:\n",
    "{generated_code}\n",
    "\n",
    "Generated Prompt:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Step 3: Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
    "\n",
    "# Step 4: Function to generate prompts based on Python code\n",
    "def generate_prompts_for_outer_code(generated_code_folder, real_examples):\n",
    "    generated_prompts = {}\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Iterate over all .py files in the generated code folder\n",
    "    for py_file in os.listdir(generated_code_folder):\n",
    "        if py_file.endswith('.py'):\n",
    "            file_path = os.path.join(generated_code_folder, py_file)\n",
    "\n",
    "            # Load the generated Python code\n",
    "            with open(file_path, 'r') as f:\n",
    "                generated_code = f.read()\n",
    "\n",
    "            # Use the first two real examples as in-context examples\n",
    "            example1 = real_examples[0]\n",
    "            example2 = real_examples[1]\n",
    "\n",
    "            # Prepare the inputs for the LLM\n",
    "            inputs = {\n",
    "                \"example1_prompt\": example1['prompt'],\n",
    "                \"example1_code\": example1['code'],\n",
    "                \"example2_prompt\": example2['prompt'],\n",
    "                \"example2_code\": example2['code'],\n",
    "                \"generated_code\": generated_code,\n",
    "                \"inner_analyzer_code\": real_inner_analyzer_code\n",
    "            }\n",
    "\n",
    "            # Generate the prompt for the current Python file\n",
    "            result = chain.invoke(inputs)\n",
    "            generated_prompt = result['text'].strip()\n",
    "\n",
    "            # Store the generated prompt and the corresponding Python code in the dictionary\n",
    "            generated_prompts[generated_prompt] = generated_code\n",
    "\n",
    "    return generated_prompt\n",
    "\n",
    "generated_code_folder = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\generated_datasets\\outer_generated_dataset'\n",
    "\n",
    "# Step 6: Generate the prompts for the Python files\n",
    "generated_dataset = generate_prompts_for_outer_code(generated_code_folder, real_prompt_code_pairs)\n",
    "generated_dataset += '\\n' + real_inner_analyzer_code\n",
    "\n",
    "output_directory = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\generated_datasets\\outer_generated_dataset'\n",
    "filename = os.path.join(output_directory, f'outer_analyzer_prompt_{2}.txt')\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(generated_dataset)\n",
    "print(f\"Modified analyzer {2} saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt for modified_dl_retx_analyzer_test_1.py saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\prompt_dl_retx_analyzer_test_1.txt\n",
      "Prompt for modified_dl_retx_analyzer_test_2.py saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\prompt_dl_retx_analyzer_test_2.txt\n",
      "Prompt for modified_kpi-manager-test-experimental_1.py saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\prompt_kpi-manager-test-experimental_1.txt\n",
      "Prompt for modified_kpi-manager-test-experimental_2.py saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\prompt_kpi-manager-test-experimental_2.txt\n",
      "Prompt for modified_kpi-manager-test_1.py saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\prompt_kpi-manager-test_1.txt\n",
      "Prompt for modified_kpi-manager-test_2.py saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\prompt_kpi-manager-test_2.txt\n",
      "Prompt for modified_lte-measurement-example_1.py saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\prompt_lte-measurement-example_1.txt\n",
      "Prompt for modified_lte-measurement-example_2.py saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\prompt_lte-measurement-example_2.txt\n",
      "Prompt for modified_lte-nas-layer-example_1.py saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\prompt_lte-nas-layer-example_1.txt\n",
      "Prompt for modified_lte-nas-layer-example_2.py saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\prompt_lte-nas-layer-example_2.txt\n",
      "Prompt for modified_monitor-example_1.py saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\prompt_monitor-example_1.txt\n",
      "Prompt for modified_monitor-example_2.py saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\prompt_monitor-example_2.txt\n",
      "Prompt for modified_msg-statistics-example_1.py saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\prompt_msg-statistics-example_1.txt\n",
      "Prompt for modified_msg-statistics-example_2.py saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\prompt_msg-statistics-example_2.txt\n",
      "Prompt for modified_offline-latency-analysis-ul_1.py saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\prompt_offline-latency-analysis-ul_1.txt\n",
      "Prompt for modified_offline-latency-analysis-ul_2.py saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\prompt_offline-latency-analysis-ul_2.txt\n",
      "Prompt for modified_online-analysis-example_1.py saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\prompt_online-analysis-example_1.txt\n",
      "Prompt for modified_online-analysis-example_2.py saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset\\prompt_online-analysis-example_2.txt\n",
      "All prompts generated and saved.\n"
     ]
    }
   ],
   "source": [
    "### FULL MODIFIED OUTER GENERATION PROMPTS ANALYZER PAIRS SCRIPT\n",
    "import os\n",
    "import json\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Define a dictionary mapping inner analyzers to their corresponding outer analyzers\n",
    "analyzer_mapping = {\n",
    "    \"lte_dl_retx_analyzer.py\": [\"dl_retx_analyzer_test.py\"],\n",
    "    \"kpi/kpi_manager.py\": [\"kpi-manager-test-experimental.py\", \"kpi-manager-test.py\"],\n",
    "    \"lte_measurement_analyzer.py\": [\"lte-measurement-example.py\"],\n",
    "    \"lte_nas_analyzer.py\": [\"lte-nas-layer-example.py\"],\n",
    "    \"msg_logger.py\": [\"monitor-example.py\"],\n",
    "    \"msg_statistics.py\": [\"msg-statistics-example.py\"],\n",
    "    \"uplink_latency_analyzer.py\": [\"offline-latency-analysis-ul.py\"],\n",
    "    \"nr_rrc_analyzer.py\": [\"online-analysis-example.py\"]\n",
    "}\n",
    "\n",
    "# Load prompt-code pairs for real examples\n",
    "def load_prompt_code_pairs(directory):\n",
    "    prompt_code_pairs = []\n",
    "    for folder_name in os.listdir(directory):\n",
    "        if folder_name == 'logs':\n",
    "            continue\n",
    "        folder_path = os.path.join(directory, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            prompt_file = next((f for f in os.listdir(folder_path) if f.endswith('.txt')), None)\n",
    "            code_file = next((f for f in os.listdir(folder_path) if f.endswith('.py')), None)\n",
    "            if prompt_file and code_file:\n",
    "                with open(os.path.join(folder_path, prompt_file), 'r') as f:\n",
    "                    prompt = f.read().strip()\n",
    "                with open(os.path.join(folder_path, code_file), 'r') as f:\n",
    "                    code = f.read().strip()\n",
    "                prompt_code_pairs.append({\n",
    "                    'prompt': prompt,\n",
    "                    'code': code\n",
    "                })\n",
    "    return prompt_code_pairs\n",
    "\n",
    "# Load real prompt-code pairs\n",
    "real_examples_directory = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\testcases'\n",
    "real_prompt_code_pairs = load_prompt_code_pairs(real_examples_directory)\n",
    "\n",
    "# Define the prompt template for generating new prompts\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"example1_prompt\", \"example1_code\", \"example2_prompt\", \"example2_code\", \"inner_analyzer_code\", \"generated_code\"],\n",
    "    template=\"\"\"\n",
    "Below are two examples of prompt-code pairs:\n",
    "\n",
    "Example 1:\n",
    "Prompt: {example1_prompt}\n",
    "Code:\n",
    "{example1_code}\n",
    "\n",
    "Example 2:\n",
    "Prompt: {example2_prompt}\n",
    "Code:\n",
    "{example2_code}\n",
    "\n",
    "The prompts are used by an LLM to generate the code. However, I have a different need. \n",
    "There are 2 analyzer files: an outer analyzer file and inner analyzer file. The inner analyzer file uses the Mobileinsight library and Analyzer class definitions to create \\\n",
    "a custom Analyzer class, and the outer analyzer file uses this custom Analyzer class contained in the inner analyzer file to write a script that will evaluate some metrics.\n",
    "Instead of having a prompt like the above 2 example prompts that don't give any analyzer file (inner or outer) and only provides instructions on the new analyzer file, \\\n",
    "I need a prompt that can give the code for an inner analyzer file, and instructions on what is required for an outer analyzer file. This prompt will then be used to generate \\\n",
    "a new outer analyzer file.\n",
    "\n",
    "Your task is the generate a prompt for the given outer analyzer file below.\\\n",
    "I will be giving you the outer analyzer file, as well as the inner analyzer file that was used to make it, and your task is to create a prompt with instructions on what the \\\n",
    "outer analyzer file should do, noting that it should be using the inner analyzer file and following a similar style to the 2 examples above. \\\n",
    "You don't need to provide the code for the inner analyzer file; I will be taking your output and appending the inner analyzer file code to it.\n",
    "\n",
    "Inner analyzer code:\n",
    "{inner_analyzer_code}\n",
    "\n",
    "Outer analyzer code:\n",
    "{generated_code}\n",
    "\n",
    "Generated Prompt:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
    "\n",
    "# Function to determine the corresponding inner analyzer file based on outer analyzer filename\n",
    "def find_inner_analyzer_filename(outer_filename_base):\n",
    "    for inner_analyzer, outer_analyzers in analyzer_mapping.items():\n",
    "        for outer_analyzer in outer_analyzers:\n",
    "            if outer_filename_base.startswith(os.path.splitext(outer_analyzer)[0]):\n",
    "                return inner_analyzer\n",
    "    return None\n",
    "\n",
    "# Generate prompts for the outer code files\n",
    "def generate_prompts_for_outer_code(inner_analyzer_folder, generated_code_folder, real_examples):\n",
    "    generated_prompts = {}\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Iterate over all .py files in the generated code folder\n",
    "    for py_file in os.listdir(generated_code_folder):\n",
    "        if py_file.endswith('.py') and py_file.startswith(\"modified_\"):\n",
    "            file_path = os.path.join(generated_code_folder, py_file)\n",
    "            outer_filename_base = \"_\".join(py_file.split(\"_\")[1:]).split(\".\")[0]\n",
    "\n",
    "            # Find the inner analyzer filename that corresponds to the base name of the outer analyzer\n",
    "            inner_analyzer = find_inner_analyzer_filename(outer_filename_base)\n",
    "            if inner_analyzer is None:\n",
    "                print(f\"No matching inner analyzer found for outer analyzer: {outer_filename_base}\")\n",
    "                continue\n",
    "\n",
    "            # Load the inner analyzer code\n",
    "            inner_analyzer_path = os.path.join(inner_analyzer_folder, inner_analyzer)\n",
    "            if not os.path.exists(inner_analyzer_path):\n",
    "                print(f\"Inner analyzer file not found: {inner_analyzer}\")\n",
    "                continue\n",
    "            with open(inner_analyzer_path, 'r') as f:\n",
    "                inner_analyzer_code = f.read()\n",
    "\n",
    "            # Load the generated outer analyzer code\n",
    "            with open(file_path, 'r') as f:\n",
    "                generated_code = f.read()\n",
    "\n",
    "            # Use the first two real examples as in-context examples\n",
    "            example1 = real_examples[0]\n",
    "            example2 = real_examples[1]\n",
    "\n",
    "            # Prepare inputs for the LLM\n",
    "            inputs = {\n",
    "                \"example1_prompt\": example1['prompt'],\n",
    "                \"example1_code\": example1['code'],\n",
    "                \"example2_prompt\": example2['prompt'],\n",
    "                \"example2_code\": example2['code'],\n",
    "                \"inner_analyzer_code\": inner_analyzer_code,\n",
    "                \"generated_code\": generated_code\n",
    "            }\n",
    "\n",
    "            # Generate the prompt\n",
    "            result = chain.invoke(inputs)\n",
    "            generated_prompt = result['text'].strip() + '\\n' + inner_analyzer_code\n",
    "\n",
    "            # Store the generated prompt\n",
    "            generated_prompts[generated_prompt] = generated_code\n",
    "\n",
    "            # Save the generated prompt to a file\n",
    "            output_directory = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset'\n",
    "            filename = os.path.join(output_directory, f'prompt_{outer_filename_base}.txt')\n",
    "            with open(filename, 'w') as f:\n",
    "                f.write(generated_prompt)\n",
    "            print(f\"Prompt for {py_file} saved to {filename}\")\n",
    "\n",
    "    return generated_prompts\n",
    "\n",
    "real_inner_analyzer_folder = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\mobile_insight\\analyzer'\n",
    "# Define the folder containing generated outer analyzer code files\n",
    "generated_code_folder = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\genevouter_generated_dataset'\n",
    "\n",
    "# Generate prompts for each outer analyzer file\n",
    "generated_dataset = generate_prompts_for_outer_code(real_inner_analyzer_folder, generated_code_folder, real_prompt_code_pairs)\n",
    "print(\"All prompts generated and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODIFIED GENERATION INNER ANALYZER SCRIPT\n",
    "\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# Step 1: Load and chunk the codebase, then create embeddings\n",
    "def index_codebase(directory):\n",
    "    # Load all .py files from the directory\n",
    "    loader = DirectoryLoader(\n",
    "        path=directory,\n",
    "        glob=\"**/*.py\",  # Only .py files\n",
    "        exclude=[\"**/__pycache__/**\", \"**/*.pyc\"]\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        separators=['\\n\\n', '\\n', ' ', '']\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # Create embeddings for each chunk\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = Chroma.from_documents(chunks, embeddings)\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "# Index the codebase\n",
    "codebase_directory = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\mobile_insight'\n",
    "vectorstore = index_codebase(codebase_directory)\n",
    "\n",
    "# Step 2: Load a real example analyzer code\n",
    "def load_real_analyzer(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        real_analyzer_code = f.read()\n",
    "    return real_analyzer_code\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Create the prompt template for modifying the real analyzer code\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"real_outer_analyzer_code\", \"real_inner_analyzer_code\", \"retrieved_context\"],\n",
    "    template=\"\"\"\n",
    "You are writing modified example python files that use an open-source project library called Mobileinsight.\n",
    "There are 2 analyzer files: an outer analyzer file and inner analyzer file. The inner analyzer file uses the Mobileinsight library and Analyzer class definitions to create \\\n",
    "a custom Analyzer class, and the outer analyzer file uses this custom Analyzer class contained in the inner analyzer file to write a script that will evaluate some metrics.\n",
    "You will be given both the outer analyzer file and the inner analyzer file being used by that outer analyzer file, and you will need to use the same outer analyzer file to \\\n",
    "create a slightly modified inner analyzer file. The new inner analyzer should perform a similar analysis but with some changes, such as adjusting metrics, altering data processing,\n",
    "or applying a different calculation. Ensure the modified analyzer remains functional and consistent with the codebase's style and structure.\n",
    "You do NOT need to make any drastic changes; adding some slightly altered output through modified calculations should be enough.\n",
    "\n",
    "Below are relevant parts of the Python codebase that provide useful context:\n",
    "\n",
    "{retrieved_context}\n",
    "\n",
    "Here is the example of an existing outer analyzer file from the codebase:\n",
    "\n",
    "Real Outer Analyzer Code:\n",
    "{real_outer_analyzer_code}\n",
    "\n",
    "Here is the example of an existing inner analyzer Python from the codebase:\n",
    "\n",
    "Real Inner Analyzer Code:\n",
    "{real_inner_analyzer_code}\n",
    "\n",
    "NOTE: ONLY PROVIDE PYTHON CODE, DO NOT ADD ANY OTHER TEXT BEFORE OR AFTER AS THIS OUTPUT IS BEING SAVED DIRECTLY INTO A PY FILE.\n",
    "\n",
    "Generated Modified Inner Analyzer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Step 4: Initialize the LLM\n",
    "llm = ChatOpenAI(model_name='gpt-4o', temperature=0.7)\n",
    "\n",
    "def generate_modified_inner_analyzers(num_examples, vectorstore, real_outer_analyzer_code, real_inner_analyzer_code):\n",
    "    modified_analyzers = []\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    \n",
    "    for _ in range(num_examples):\n",
    "        # Retrieve relevant code snippets\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "        relevant_docs = retriever.get_relevant_documents(real_outer_analyzer_code+real_inner_analyzer_code)\n",
    "        retrieved_context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "        \n",
    "        # Prepare the inputs for the LLM\n",
    "        inputs = {\n",
    "            \"real_outer_analyzer_code\": real_outer_analyzer_code,\n",
    "            \"real_inner_analyzer_code\": real_inner_analyzer_code,\n",
    "            \"retrieved_context\": retrieved_context\n",
    "        }\n",
    "\n",
    "        # Generate the modified analyzer code\n",
    "        result = chain.invoke(inputs)\n",
    "        modified_analyzer_code = result['text'].strip()\n",
    "\n",
    "        # Append the modified analyzer code to the list\n",
    "        modified_analyzers.append(modified_analyzer_code)\n",
    "\n",
    "    return modified_analyzers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified analyzer saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\geneinner_generated_dataset\\modified_lte_dl_retx_analyzer_1.py\n",
      "Modified analyzer saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\geneinner_generated_dataset\\modified_lte_dl_retx_analyzer_2.py\n",
      "Modified analyzer saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\geneinner_generated_dataset\\modified_kpi\\kpi_manager_1.py\n",
      "Modified analyzer saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\geneinner_generated_dataset\\modified_kpi\\kpi_manager_2.py\n",
      "Modified analyzer saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\geneinner_generated_dataset\\modified_kpi\\kpi_manager_2_1.py\n",
      "Modified analyzer saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\geneinner_generated_dataset\\modified_kpi\\kpi_manager_2_2.py\n",
      "Modified analyzer saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\geneinner_generated_dataset\\modified_lte_measurement_analyzer_1.py\n",
      "Modified analyzer saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\geneinner_generated_dataset\\modified_lte_measurement_analyzer_2.py\n"
     ]
    }
   ],
   "source": [
    "## GENERATE ALL OUTER ANALYZERS\n",
    "\n",
    "analyzer_files = {\n",
    "    \"lte_dl_retx_analyzer.py\": [\"dl_retx_analyzer_test.py\"],\n",
    "    \"kpi\\\\kpi_manager.py\": [\"kpi-manager-test-experimental.py\", \"kpi-manager-test.py\"],\n",
    "    \"lte_measurement_analyzer.py\": [\"lte-measurement-example.py\"],\n",
    "    \"lte_nas_analyzer.py\": [\"lte-nas-layer-example.py\"],\n",
    "    \"msg_logger.py\": [\"monitor-example.py\"],\n",
    "    \"msg_statistics.py\": [\"msg-statistics-example.py\"],\n",
    "    \"uplink_latency_analyzer.py\": [\"offline-latency-analysis-ul.py\"],\n",
    "    \"nr_rrc_analyzer.py\": [\"online-analysis-example.py\"]\n",
    "}\n",
    "\n",
    "# Define paths for analyzers\n",
    "real_examples_folder = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\mobile_insight\\examples'\n",
    "real_inner_analyzer_folder = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\mobile_insight\\analyzer'\n",
    "output_directory = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\geneinner_generated_dataset'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Step 6: Loop through the dictionary and generate modified outer analyzers\n",
    "for inner_analyzer, outer_analyzers in analyzer_files.items():\n",
    "    i = 0\n",
    "    # Load the inner analyzer code\n",
    "    inner_analyzer_path = os.path.join(real_inner_analyzer_folder, inner_analyzer)\n",
    "    real_inner_analyzer_code = load_real_analyzer(inner_analyzer_path)\n",
    "\n",
    "    # Generate modified outer analyzers for each outer analyzer associated with this inner analyzer\n",
    "    for outer_analyzer in outer_analyzers:\n",
    "        outer_analyzer_path = os.path.join(real_examples_folder, outer_analyzer)\n",
    "        real_outer_analyzer_code = load_real_analyzer(outer_analyzer_path)\n",
    "\n",
    "        # Generate modified code\n",
    "        modified_codes = generate_modified_inner_analyzers(2, vectorstore, real_outer_analyzer_code, real_inner_analyzer_code)\n",
    "\n",
    "        # Save each modified analyzer\n",
    "        for idx, modified_code in enumerate(modified_codes):\n",
    "            if i == 1:\n",
    "                filename = f\"modified_{os.path.splitext(inner_analyzer)[0]}_2_{idx + 1}.py\"\n",
    "            else:\n",
    "                filename = f\"modified_{os.path.splitext(inner_analyzer)[0]}_{idx + 1}.py\"\n",
    "            output_path = os.path.join(output_directory, filename)\n",
    "            code = modified_code.replace(\"```python\", \"\", 1)\n",
    "            code = code.replace(\"```\", \"\", 1)\n",
    "            with open(output_path, 'w') as f:\n",
    "                f.write(code)\n",
    "            print(f\"Modified analyzer saved to {output_path}\")\n",
    "        i += 1\n",
    "\n",
    "print(\"All modified analyzers generated and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one example real analyzer code\n",
    "real_outer_analyzer_path = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\mobile_insight\\examples\\offline-latency-analysis-ul.py'\n",
    "real_outer_analyzer_code = load_real_analyzer(real_outer_analyzer_path)\n",
    "\n",
    "real_inner_analyzer_path = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\mobile_insight\\analyzer\\uplink_latency_analyzer.py'\n",
    "real_inner_analyzer_code = load_real_analyzer(real_inner_analyzer_path)\n",
    "\n",
    "msg_stat_ex = generate_modified_outer_analyzers(1, vectorstore, real_outer_analyzer_code, real_inner_analyzer_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified analyzer 2 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\generated_datasets\\inner_generated_dataset\\inner_analyzer_2.py\n"
     ]
    }
   ],
   "source": [
    "output_directory = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\generated_datasets\\inner_generated_dataset'\n",
    "filename = os.path.join(output_directory, f'inner_analyzer_{2}.py')\n",
    "code = msg_stat_ex[0].replace(\"```python\", \"\", 1)\n",
    "code = code.replace(\"```\", \"\", 1)\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(code)\n",
    "print(f\"Modified analyzer {2} saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified analyzer 2 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\generated_datasets\\inner_generated_dataset\\inner_analyzer_prompt_2.txt\n"
     ]
    }
   ],
   "source": [
    "### SINGLE MODIFIED OUTER GENERATION PROMPTS ANALYZER PAIRS SCRIPT\n",
    "\n",
    "import json\n",
    "\n",
    "# Step 1: Load real prompt-code pairs from folders\n",
    "def load_prompt_code_pairs(directory):\n",
    "    prompt_code_pairs = []\n",
    "    \n",
    "    # Each subdirectory contains a prompt (txt) and code (py) file\n",
    "    for folder_name in os.listdir(directory):\n",
    "        if folder_name == 'logs':\n",
    "            continue\n",
    "        folder_path = os.path.join(directory, folder_name)\n",
    "        \n",
    "        if os.path.isdir(folder_path):\n",
    "            # Find the txt and py files\n",
    "            prompt_file = next((f for f in os.listdir(folder_path) if f.endswith('.txt')), None)\n",
    "            code_file = next((f for f in os.listdir(folder_path) if f.endswith('.py')), None)\n",
    "            \n",
    "            if prompt_file and code_file:\n",
    "                # Load the prompt\n",
    "                with open(os.path.join(folder_path, prompt_file), 'r') as f:\n",
    "                    prompt = f.read().strip()\n",
    "                \n",
    "                # Load the code\n",
    "                with open(os.path.join(folder_path, code_file), 'r') as f:\n",
    "                    code = f.read().strip()\n",
    "                \n",
    "                # Append to the list as a dict\n",
    "                prompt_code_pairs.append({\n",
    "                    'prompt': prompt,\n",
    "                    'code': code\n",
    "                })\n",
    "    \n",
    "    return prompt_code_pairs\n",
    "\n",
    "# Load real prompt-code pairs\n",
    "real_examples_directory = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\testcases'\n",
    "real_prompt_code_pairs = load_prompt_code_pairs(real_examples_directory)\n",
    "\n",
    "# Step 2: Create the prompt template for generating new prompts based on Python code\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"example1_prompt\", \"example1_code\", \"example2_prompt\", \"example2_code\", \"outer_analyzer_code\", \"generated_code\"],\n",
    "    template=\"\"\"\n",
    "Below are two examples of prompt-code pairs:\n",
    "\n",
    "Example 1:\n",
    "Prompt: {example1_prompt}\n",
    "Code:\n",
    "{example1_code}\n",
    "\n",
    "Example 2:\n",
    "Prompt: {example2_prompt}\n",
    "Code:\n",
    "{example2_code}\n",
    "\n",
    "The prompts are used by an LLM to generate the code. However, I have a different need. \n",
    "There are 2 analyzer files: an outer analyzer file and inner analyzer file. The inner analyzer file uses the Mobileinsight library and Analyzer class definitions to create \\\n",
    "a custom Analyzer class, and the outer analyzer file uses this custom Analyzer class contained in the inner analyzer file to write a script that will evaluate some metrics.\n",
    "Instead of having a prompt like the above 2 example prompts that don't give any analyzer file (inner or outer) and only provides instructions on the new analyzer file, \\\n",
    "I need a prompt that can give the code for an outer analyzer file, and instructions on what is required for an inner analyzer file. This prompt will then be used to generate \\\n",
    "a new inner analyzer file.\n",
    "\n",
    "Your task is the generate a prompt for the given inner analyzer file below.\\\n",
    "I will be giving you the outer analyzer file, as well as the inner analyzer file that was used to make it, and your task is to create a prompt with instructions on what the \\\n",
    "inner analyzer file should do, noting that it will be used to run the outer analyzer file and following a similar style to the 2 examples above. \\\n",
    "You don't need to provide the code for the outer analyzer file; I will be taking your output and appending the outer analyzer file code to it.\n",
    "\n",
    "Outer analyzer code:\n",
    "{outer_analyzer_code}\n",
    "\n",
    "Inner analyzer code:\n",
    "{generated_code}\n",
    "\n",
    "Generated Prompt:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Step 3: Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
    "\n",
    "# Step 4: Function to generate prompts based on Python code\n",
    "def generate_prompts_for_inner_code(generated_code_folder, real_examples):\n",
    "    generated_prompts = {}\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Iterate over all .py files in the generated code folder\n",
    "    for py_file in os.listdir(generated_code_folder):\n",
    "        if py_file.endswith('.py'):\n",
    "            file_path = os.path.join(generated_code_folder, py_file)\n",
    "\n",
    "            # Load the generated Python code\n",
    "            with open(file_path, 'r') as f:\n",
    "                generated_code = f.read()\n",
    "\n",
    "            # Use the first two real examples as in-context examples\n",
    "            example1 = real_examples[0]\n",
    "            example2 = real_examples[1]\n",
    "\n",
    "            # Prepare the inputs for the LLM\n",
    "            inputs = {\n",
    "                \"example1_prompt\": example1['prompt'],\n",
    "                \"example1_code\": example1['code'],\n",
    "                \"example2_prompt\": example2['prompt'],\n",
    "                \"example2_code\": example2['code'],\n",
    "                \"generated_code\": generated_code,\n",
    "                \"outer_analyzer_code\": real_outer_analyzer_code\n",
    "            }\n",
    "\n",
    "            # Generate the prompt for the current Python file\n",
    "            result = chain.invoke(inputs)\n",
    "            generated_prompt = result['text'].strip()\n",
    "\n",
    "            # Store the generated prompt and the corresponding Python code in the dictionary\n",
    "            generated_prompts[generated_prompt] = generated_code\n",
    "\n",
    "    return generated_prompt\n",
    "\n",
    "generated_code_folder = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\generated_datasets\\inner_generated_dataset'\n",
    "\n",
    "# Step 6: Generate the prompts for the Python files\n",
    "generated_dataset = generate_prompts_for_inner_code(generated_code_folder, real_prompt_code_pairs)\n",
    "generated_dataset += '\\n' + real_outer_analyzer_code\n",
    "\n",
    "output_directory = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\generated_datasets\\inner_generated_dataset'\n",
    "filename = os.path.join(output_directory, f'inner_analyzer_prompt_{2}.txt')\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(generated_dataset)\n",
    "print(f\"Modified analyzer {2} saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Define a dictionary mapping outer analyzers to their corresponding inner analyzers\n",
    "analyzer_mapping = {\n",
    "    \"dl_retx_analyzer_test.py\": [\"lte_dl_retx_analyzer.py\"],\n",
    "    \"kpi-manager-test-experimental.py\": [\"kpi/kpi_manager.py\"],\n",
    "    \"kpi-manager-test.py\": [\"kpi/kpi_manager.py\"],\n",
    "    \"lte-measurement-example.py\": [\"lte_measurement_analyzer.py\"],\n",
    "    \"lte-nas-layer-example.py\": [\"lte_nas_analyzer.py\"],\n",
    "    \"monitor-example.py\": [\"msg_logger.py\"],\n",
    "    \"msg-statistics-example.py\": [\"msg_statistics.py\"],\n",
    "    \"offline-latency-analysis-ul.py\": [\"uplink_latency_analyzer.py\"],\n",
    "    \"online-analysis-example.py\": [\"nr_rrc_analyzer.py\"]\n",
    "}\n",
    "\n",
    "# Load prompt-code pairs for real examples\n",
    "def load_prompt_code_pairs(directory):\n",
    "    prompt_code_pairs = []\n",
    "    for folder_name in os.listdir(directory):\n",
    "        if folder_name == 'logs':\n",
    "            continue\n",
    "        folder_path = os.path.join(directory, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            prompt_file = next((f for f in os.listdir(folder_path) if f.endswith('.txt')), None)\n",
    "            code_file = next((f for f in os.listdir(folder_path) if f.endswith('.py')), None)\n",
    "            if prompt_file and code_file:\n",
    "                with open(os.path.join(folder_path, prompt_file), 'r') as f:\n",
    "                    prompt = f.read().strip()\n",
    "                with open(os.path.join(folder_path, code_file), 'r') as f:\n",
    "                    code = f.read().strip()\n",
    "                prompt_code_pairs.append({\n",
    "                    'prompt': prompt,\n",
    "                    'code': code\n",
    "                })\n",
    "    return prompt_code_pairs\n",
    "\n",
    "# Load real prompt-code pairs\n",
    "real_examples_directory = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\testcases'\n",
    "real_prompt_code_pairs = load_prompt_code_pairs(real_examples_directory)\n",
    "\n",
    "# Define the prompt template for generating new prompts\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"example1_prompt\", \"example1_code\", \"example2_prompt\", \"example2_code\", \"outer_analyzer_code\", \"generated_code\"],\n",
    "    template=\"\"\"\n",
    "Below are two examples of prompt-code pairs:\n",
    "\n",
    "Example 1:\n",
    "Prompt: {example1_prompt}\n",
    "Code:\n",
    "{example1_code}\n",
    "\n",
    "Example 2:\n",
    "Prompt: {example2_prompt}\n",
    "Code:\n",
    "{example2_code}\n",
    "\n",
    "The prompts are used by an LLM to generate the code. However, I have a different need. \n",
    "There are 2 analyzer files: an outer analyzer file and inner analyzer file. The inner analyzer file uses the Mobileinsight library and Analyzer class definitions to create \\\n",
    "a custom Analyzer class, and the outer analyzer file uses this custom Analyzer class contained in the inner analyzer file to write a script that will evaluate some metrics.\n",
    "Instead of having a prompt like the above 2 example prompts that don't give any analyzer file (inner or outer) and only provides instructions on the new analyzer file, \\\n",
    "I need a prompt that can give the code for an outer analyzer file, and instructions on what is required for an inner analyzer file. This prompt will then be used to generate \\\n",
    "a new inner analyzer file.\n",
    "\n",
    "Your task is to generate a prompt for the given inner analyzer file below.\\\n",
    "I will be giving you the outer analyzer file, as well as the inner analyzer file that was used to make it, and your task is to create a prompt with instructions on what the \\\n",
    "inner analyzer file should do, noting that it will be used to run the outer analyzer file and following a similar style to the 2 examples above. \\\n",
    "You don't need to provide the code for the outer analyzer file; I will be taking your output and appending the outer analyzer file code to it.\n",
    "\n",
    "Outer analyzer code:\n",
    "{outer_analyzer_code}\n",
    "\n",
    "Inner analyzer code:\n",
    "{generated_code}\n",
    "\n",
    "Generated Prompt:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
    "\n",
    "# Function to determine the corresponding outer analyzer file based on inner analyzer filename\n",
    "def find_outer_analyzer_filename(inner_filename_base):\n",
    "    for outer_analyzer, inner_analyzers in analyzer_mapping.items():\n",
    "        for inner_analyzer in inner_analyzers:\n",
    "            if inner_filename_base.startswith(os.path.splitext(inner_analyzer)[0]):\n",
    "                return outer_analyzer\n",
    "    return None\n",
    "\n",
    "# Generate prompts for the inner code files\n",
    "def generate_prompts_for_inner_code(outer_analyzer_folder, generated_code_folder, real_examples):\n",
    "    generated_prompts = {}\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Iterate over all .py files in the generated code folder\n",
    "    for py_file in os.listdir(generated_code_folder):\n",
    "        if py_file.endswith('.py') and py_file.startswith(\"modified_\"):\n",
    "            file_path = os.path.join(generated_code_folder, py_file)\n",
    "            inner_filename_base = \"_\".join(py_file.split(\"_\")[1:]).split(\".\")[0]\n",
    "\n",
    "            # Find the outer analyzer filename that corresponds to the base name of the inner analyzer\n",
    "            outer_analyzer = find_outer_analyzer_filename(inner_filename_base)\n",
    "            if outer_analyzer is None:\n",
    "                print(f\"No matching outer analyzer found for inner analyzer: {inner_filename_base}\")\n",
    "                continue\n",
    "\n",
    "            # Load the outer analyzer code\n",
    "            outer_analyzer_path = os.path.join(outer_analyzer_folder, outer_analyzer)\n",
    "            if not os.path.exists(outer_analyzer_path):\n",
    "                print(f\"Outer analyzer file not found: {outer_analyzer}\")\n",
    "                continue\n",
    "            with open(outer_analyzer_path, 'r') as f:\n",
    "                outer_analyzer_code = f.read()\n",
    "\n",
    "            # Load the generated inner analyzer code\n",
    "            with open(file_path, 'r') as f:\n",
    "                generated_code = f.read()\n",
    "\n",
    "            # Use the first two real examples as in-context examples\n",
    "            example1 = real_examples[0]\n",
    "            example2 = real_examples[1]\n",
    "\n",
    "            # Prepare inputs for the LLM\n",
    "            inputs = {\n",
    "                \"example1_prompt\": example1['prompt'],\n",
    "                \"example1_code\": example1['code'],\n",
    "                \"example2_prompt\": example2['prompt'],\n",
    "                \"example2_code\": example2['code'],\n",
    "                \"outer_analyzer_code\": outer_analyzer_code,\n",
    "                \"generated_code\": generated_code\n",
    "            }\n",
    "\n",
    "            # Generate the prompt\n",
    "            result = chain.invoke(inputs)\n",
    "            generated_prompt = result['text'].strip()\n",
    "\n",
    "            # Store the generated prompt\n",
    "            generated_prompts[generated_prompt] = generated_code\n",
    "\n",
    "            # Save the generated prompt to a file\n",
    "            output_directory = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\generated_datasets\\geneinner_generated_dataset'\n",
    "            filename = os.path.join(output_directory, f'prompt_{inner_filename_base}.txt')\n",
    "            with open(filename, 'w') as f:\n",
    "                f.write(generated_prompt)\n",
    "            print(f\"Prompt for {py_file} saved to {filename}\")\n",
    "\n",
    "    return generated_prompts\n",
    "\n",
    "real_outer_analyzer_folder = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\mobile_insight\\examples'\n",
    "# Define the folder containing generated inner analyzer code files\n",
    "generated_code_folder = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\generated_datasets\\geneinner_generated_dataset'\n",
    "\n",
    "# Generate prompts for each inner analyzer file\n",
    "generated_dataset = generate_prompts_for_inner_code(real_outer_analyzer_folder, real_prompt_code_pairs)\n",
    "print(\"All prompts generated and saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
