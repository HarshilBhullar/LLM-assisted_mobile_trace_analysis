{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPEN_AI_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FULLY GENERATED ANALYZER SCRIPT\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "# Step 1: Load the code files from the repository\n",
    "loader = DirectoryLoader(\n",
    "    path=r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\mobile_insight',\n",
    "    glob='**/*.py',             # Only .py files\n",
    "    exclude=['**/__pycache__/**', '**/*.pyc']  # Exclude cache and .pyc files\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "# Step 2: Split the code files into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    separators=['\\n\\n', '\\n', ' ', '']\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Step 3: Create embeddings for the chunks\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Step 4: Store the embeddings in a vectorstore\n",
    "vectorstore = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "def get_random_context(k=5):\n",
    "    # Randomly select k documents from the list of texts\n",
    "    selected_docs = random.sample(texts, min(k, len(texts)))\n",
    "    context = '\\n\\n'.join([doc.page_content for doc in selected_docs])\n",
    "    return context\n",
    "\n",
    "# Step 6: Read the two example files\n",
    "with open(r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\testcases\\attach_stats\\attach_stats.py', 'r') as f:\n",
    "    example1 = f.read()\n",
    "\n",
    "with open(r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\testcases\\interval_stats\\interval_stats.py', 'r') as f:\n",
    "    example2 = f.read()\n",
    "\n",
    "# Step 7: Create the prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['example1', 'example2', 'context'],\n",
    "    template=\"\"\"\n",
    "You are an AI assistant that generates code for analyzers using the given Python library. Here are two examples of analyzer code:\n",
    "\n",
    "Example 1:\n",
    "{example1}\n",
    "\n",
    "Example 2:\n",
    "{example2}\n",
    "\n",
    "Using the above examples as a guide, please generate a new analyzer code that is different from the examples provided. \n",
    "The new analyzer should demonstrate a unique functionality or feature of the library.\n",
    "NOTE: ONLY PROVIDE PYTHON CODE, DO NOT ADD ANY OTHER TEXT BEFORE OR AFTER AS THIS OUTPUT IS BEING SAVED DIRECTLY INTO A PY FILE.\n",
    "\n",
    "You have access to the following relevant code snippets from the library:\n",
    "{context}\n",
    "\n",
    "Please make sure to properly use the library's functions and classes as per the context provided.\n",
    "\n",
    "Generated Code:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Step 8: Initialize the LLM\n",
    "llm = ChatOpenAI(model_name='gpt-4o', temperature=0.7)\n",
    "\n",
    "\n",
    "# Step 9: Define the function to generate code examples\n",
    "def generate_code_examples(num_examples):\n",
    "    generated_examples = []\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    \n",
    "    for _ in range(num_examples):\n",
    "        # Retrieve random context documents\n",
    "        context = get_random_context(k=5)\n",
    "        \n",
    "        # Prepare the inputs to the prompt\n",
    "        inputs = {\n",
    "            'example1': example1,\n",
    "            'example2': example2,\n",
    "            'context': context\n",
    "        }\n",
    "        \n",
    "        # Run the chain to generate code\n",
    "        result = chain.invoke(inputs)\n",
    "        \n",
    "        # Append the generated code to the list\n",
    "        generated_code = result['text'].strip()\n",
    "        \n",
    "        # Append the generated code to the list\n",
    "        generated_examples.append(generated_code)\n",
    "        \n",
    "    return generated_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Code 4 saved to generated_analyzer_24.py\n",
      "Generated Code 5 saved to generated_analyzer_25.py\n",
      "Generated Code 6 saved to generated_analyzer_26.py\n",
      "Generated Code 7 saved to generated_analyzer_27.py\n",
      "Generated Code 8 saved to generated_analyzer_28.py\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Generate multiple code examples\n",
    "num_examples_to_generate = 5  # Adjust the number as needed\n",
    "generated_codes = generate_code_examples(num_examples_to_generate)\n",
    "\n",
    "# Step 11: Save the generated codes to files or print them\n",
    "for idx, code in enumerate(generated_codes):\n",
    "    filename = f'generated_analyzer_{23+idx+1}.py'\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(code)\n",
    "    print(f\"Generated Code {3+idx+1} saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic dataset created and saved.\n"
     ]
    }
   ],
   "source": [
    "### GENERATED PROMPTS ANALYZER PAIRS SCRIPT\n",
    "\n",
    "import json\n",
    "\n",
    "# Step 1: Load real prompt-code pairs from folders\n",
    "def load_prompt_code_pairs(directory):\n",
    "    prompt_code_pairs = []\n",
    "    \n",
    "    # Each subdirectory contains a prompt (txt) and code (py) file\n",
    "    for folder_name in os.listdir(directory):\n",
    "        if folder_name == 'logs':\n",
    "            continue\n",
    "        folder_path = os.path.join(directory, folder_name)\n",
    "        \n",
    "        if os.path.isdir(folder_path):\n",
    "            # Find the txt and py files\n",
    "            prompt_file = next((f for f in os.listdir(folder_path) if f.endswith('.txt')), None)\n",
    "            code_file = next((f for f in os.listdir(folder_path) if f.endswith('.py')), None)\n",
    "            \n",
    "            if prompt_file and code_file:\n",
    "                # Load the prompt\n",
    "                with open(os.path.join(folder_path, prompt_file), 'r') as f:\n",
    "                    prompt = f.read().strip()\n",
    "                \n",
    "                # Load the code\n",
    "                with open(os.path.join(folder_path, code_file), 'r') as f:\n",
    "                    code = f.read().strip()\n",
    "                \n",
    "                # Append to the list as a dict\n",
    "                prompt_code_pairs.append({\n",
    "                    'prompt': prompt,\n",
    "                    'code': code\n",
    "                })\n",
    "    \n",
    "    return prompt_code_pairs\n",
    "\n",
    "# Load real prompt-code pairs\n",
    "real_examples_directory = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\testcases'\n",
    "real_prompt_code_pairs = load_prompt_code_pairs(real_examples_directory)\n",
    "\n",
    "# Step 2: Create the prompt template for generating new prompts based on Python code\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"example1_prompt\", \"example1_code\", \"example2_prompt\", \"example2_code\", \"generated_code\"],\n",
    "    template=\"\"\"\n",
    "Below are two examples of prompt-code pairs:\n",
    "\n",
    "Example 1:\n",
    "Prompt: {example1_prompt}\n",
    "Code:\n",
    "{example1_code}\n",
    "\n",
    "Example 2:\n",
    "Prompt: {example2_prompt}\n",
    "Code:\n",
    "{example2_code}\n",
    "\n",
    "Given the code below, generate a prompt for it following the structure of the 2 examples:\n",
    "\n",
    "Code:\n",
    "{generated_code}\n",
    "\n",
    "Generated Prompt:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Step 3: Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
    "\n",
    "# Step 4: Function to generate prompts based on Python code\n",
    "def generate_prompts_for_code(generated_code_folder, real_examples):\n",
    "    generated_prompts = {}\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Iterate over all .py files in the generated code folder\n",
    "    for py_file in os.listdir(generated_code_folder):\n",
    "        if py_file.endswith('.py'):\n",
    "            file_path = os.path.join(generated_code_folder, py_file)\n",
    "\n",
    "            # Load the generated Python code\n",
    "            with open(file_path, 'r') as f:\n",
    "                generated_code = f.read()\n",
    "\n",
    "            # Use the first two real examples as in-context examples\n",
    "            example1 = real_examples[0]\n",
    "            example2 = real_examples[1]\n",
    "\n",
    "            # Prepare the inputs for the LLM\n",
    "            inputs = {\n",
    "                \"example1_prompt\": example1['prompt'],\n",
    "                \"example1_code\": example1['code'],\n",
    "                \"example2_prompt\": example2['prompt'],\n",
    "                \"example2_code\": example2['code'],\n",
    "                \"generated_code\": generated_code\n",
    "            }\n",
    "\n",
    "            # Generate the prompt for the current Python file\n",
    "            result = chain.invoke(inputs)\n",
    "            generated_prompt = result['text'].strip()\n",
    "\n",
    "            # Store the generated prompt and the corresponding Python code in the dictionary\n",
    "            generated_prompts[generated_prompt] = generated_code\n",
    "\n",
    "    return generated_prompts\n",
    "\n",
    "# Step 5: Define the folder where all generated Python files are saved\n",
    "generated_code_folder = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\generated_dataset'\n",
    "\n",
    "# Step 6: Generate the prompts for the Python files\n",
    "generated_dataset = generate_prompts_for_code(generated_code_folder, real_prompt_code_pairs)\n",
    "\n",
    "# Step 7: Save the generated dataset (optional, saving as JSON)\n",
    "with open('synthetic_dataset.json', 'w') as f:\n",
    "    json.dump(generated_dataset, f, indent=4)\n",
    "\n",
    "print(\"Synthetic dataset created and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODIFIED GENERATION ANALYZER SCRIPT\n",
    "\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# Step 1: Load and chunk the codebase, then create embeddings\n",
    "def index_codebase(directory):\n",
    "    # Load all .py files from the directory\n",
    "    loader = DirectoryLoader(\n",
    "        path=directory,\n",
    "        glob=\"**/*.py\",  # Only .py files\n",
    "        exclude=[\"**/__pycache__/**\", \"**/*.pyc\"]\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        separators=['\\n\\n', '\\n', ' ', '']\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # Create embeddings for each chunk\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = Chroma.from_documents(chunks, embeddings)\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "# Index the codebase\n",
    "codebase_directory = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\mobile_insight'\n",
    "vectorstore = index_codebase(codebase_directory)\n",
    "\n",
    "# Step 2: Load a real example analyzer code\n",
    "def load_real_analyzer(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        real_analyzer_code = f.read()\n",
    "    return real_analyzer_code\n",
    "\n",
    "# Load one example real analyzer code\n",
    "real_analyzer_path = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\testcases\\attach_stats\\attach_stats.py'  # Update with the actual path to your real analyzer file\n",
    "real_analyzer_code = load_real_analyzer(real_analyzer_path)\n",
    "\n",
    "# Step 3: Create the prompt template for modifying the real analyzer code\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"real_analyzer_code\", \"retrieved_context\"],\n",
    "    template=\"\"\"\n",
    "Below are relevant parts of a Python codebase that provide useful context:\n",
    "\n",
    "{retrieved_context}\n",
    "\n",
    "Here is an example of an existing analyzer Python file from the codebase:\n",
    "\n",
    "Real Analyzer Code:\n",
    "{real_analyzer_code}\n",
    "\n",
    "Using the codebase context and the real analyzer code as a reference, create a slightly modified version of the analyzer. \\\n",
    "    The new analyzer should perform a similar analysis but with some changes, such as adjusting metrics, altering data processing,\\\n",
    "    or applying a different calculation. Ensure the modified analyzer remains functional and consistent with the codebase's style and structure.\n",
    "\n",
    "NOTE: ONLY PROVIDE PYTHON CODE, DO NOT ADD ANY OTHER TEXT BEFORE OR AFTER AS THIS OUTPUT IS BEING SAVED DIRECTLY INTO A PY FILE.\n",
    "\n",
    "Generated Modified Analyzer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Step 4: Initialize the LLM\n",
    "llm = ChatOpenAI(model_name='gpt-4o', temperature=0.7)\n",
    "\n",
    "def generate_modified_analyzers(num_examples, vectorstore, real_analyzer_code):\n",
    "    modified_analyzers = []\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    \n",
    "    for _ in range(num_examples):\n",
    "        # Retrieve relevant code snippets\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "        relevant_docs = retriever.get_relevant_documents(real_analyzer_code)\n",
    "        retrieved_context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "        \n",
    "        # Prepare the inputs for the LLM\n",
    "        inputs = {\n",
    "            \"real_analyzer_code\": real_analyzer_code,\n",
    "            \"retrieved_context\": retrieved_context\n",
    "        }\n",
    "\n",
    "        # Generate the modified analyzer code\n",
    "        result = chain.invoke(inputs)\n",
    "        modified_analyzer_code = result['text'].strip()\n",
    "\n",
    "        # Append the modified analyzer code to the list\n",
    "        modified_analyzers.append(modified_analyzer_code)\n",
    "\n",
    "    return modified_analyzers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Modified analyzer 85 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_85.py\n",
      "Modified analyzer 86 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_86.py\n",
      "Modified analyzer 87 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_87.py\n",
      "Modified analyzer 88 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_88.py\n",
      "Modified analyzer 89 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_89.py\n",
      "Modified analyzer 90 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_90.py\n",
      "Modified analyzer 91 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_91.py\n",
      "Modified analyzer 92 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_92.py\n",
      "Modified analyzer 93 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_93.py\n",
      "Modified analyzer 94 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_94.py\n",
      "Modified analyzer 95 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_95.py\n",
      "Modified analyzer 96 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_96.py\n",
      "Modified analyzer 97 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_97.py\n",
      "Modified analyzer 98 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_98.py\n",
      "Modified analyzer 99 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_99.py\n",
      "Modified analyzer 100 saved to C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset\\modified_analyzer_100.py\n",
      "All modified analyzers generated and saved.\n"
     ]
    }
   ],
   "source": [
    "num_examples_to_generate = 2  # Adjust as needed\n",
    "real_examples_folder = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\mobile_insight\\examples'\n",
    "modified_analyzers = []\n",
    "i=0\n",
    "for py_file in os.listdir(real_examples_folder):\n",
    "    if py_file.endswith('.py'):\n",
    "        file_path = os.path.join(real_examples_folder, py_file)\n",
    "        with open(file_path, 'r') as f:\n",
    "                real_analyzer_code = f.read()\n",
    "        modified_analyzers += generate_modified_analyzers(num_examples_to_generate, vectorstore, real_analyzer_code)\n",
    "        print(i)\n",
    "        i +=1\n",
    "        if i == 8:\n",
    "             break\n",
    "\n",
    "# Step 7: Save the modified analyzers to files (optional)\n",
    "output_directory = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "for idx, code in enumerate(modified_analyzers):\n",
    "    filename = os.path.join(output_directory, f'modified_analyzer_{idx + 84 + 1}.py')\n",
    "    code = code.replace(\"```python\", \"\", 1)\n",
    "    code = code.replace(\"```\", \"\", 1)\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(code)\n",
    "    print(f\"Modified analyzer {idx + 84 + 1} saved to {filename}\")\n",
    "\n",
    "print(\"All modified analyzers generated and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified synthetic dataset created and saved.\n"
     ]
    }
   ],
   "source": [
    "### MODIFIED GENERATION PROMPTS ANALYZER PAIRS SCRIPT\n",
    "\n",
    "import json\n",
    "\n",
    "# Step 1: Load real prompt-code pairs from folders\n",
    "def load_prompt_code_pairs(directory):\n",
    "    prompt_code_pairs = []\n",
    "    \n",
    "    # Each subdirectory contains a prompt (txt) and code (py) file\n",
    "    for folder_name in os.listdir(directory):\n",
    "        if folder_name == 'logs':\n",
    "            continue\n",
    "        folder_path = os.path.join(directory, folder_name)\n",
    "        \n",
    "        if os.path.isdir(folder_path):\n",
    "            # Find the txt and py files\n",
    "            prompt_file = next((f for f in os.listdir(folder_path) if f.endswith('.txt')), None)\n",
    "            code_file = next((f for f in os.listdir(folder_path) if f.endswith('.py')), None)\n",
    "            \n",
    "            if prompt_file and code_file:\n",
    "                # Load the prompt\n",
    "                with open(os.path.join(folder_path, prompt_file), 'r') as f:\n",
    "                    prompt = f.read().strip()\n",
    "                \n",
    "                # Load the code\n",
    "                with open(os.path.join(folder_path, code_file), 'r') as f:\n",
    "                    code = f.read().strip()\n",
    "                \n",
    "                # Append to the list as a dict\n",
    "                prompt_code_pairs.append({\n",
    "                    'prompt': prompt,\n",
    "                    'code': code\n",
    "                })\n",
    "    \n",
    "    return prompt_code_pairs\n",
    "\n",
    "# Load real prompt-code pairs\n",
    "real_examples_directory = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\testcases'\n",
    "real_prompt_code_pairs = load_prompt_code_pairs(real_examples_directory)\n",
    "\n",
    "# Step 2: Create the prompt template for generating new prompts based on Python code\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"example1_prompt\", \"example1_code\", \"example2_prompt\", \"example2_code\", \"generated_code\"],\n",
    "    template=\"\"\"\n",
    "Below are two examples of prompt-code pairs:\n",
    "\n",
    "Example 1:\n",
    "Prompt: {example1_prompt}\n",
    "Code:\n",
    "{example1_code}\n",
    "\n",
    "Example 2:\n",
    "Prompt: {example2_prompt}\n",
    "Code:\n",
    "{example2_code}\n",
    "\n",
    "Given the code below, generate a prompt for it following the structure of the 2 examples:\n",
    "\n",
    "Code:\n",
    "{generated_code}\n",
    "\n",
    "Generated Prompt:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Step 3: Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
    "\n",
    "# Step 4: Function to generate prompts based on Python code\n",
    "def generate_prompts_for_code(generated_code_folder, real_examples):\n",
    "    generated_prompts = {}\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Iterate over all .py files in the generated code folder\n",
    "    for py_file in os.listdir(generated_code_folder):\n",
    "        if py_file.endswith('.py'):\n",
    "            file_path = os.path.join(generated_code_folder, py_file)\n",
    "\n",
    "            # Load the generated Python code\n",
    "            with open(file_path, 'r') as f:\n",
    "                generated_code = f.read()\n",
    "\n",
    "            # Use the first two real examples as in-context examples\n",
    "            example1 = real_examples[0]\n",
    "            example2 = real_examples[1]\n",
    "\n",
    "            # Prepare the inputs for the LLM\n",
    "            inputs = {\n",
    "                \"example1_prompt\": example1['prompt'],\n",
    "                \"example1_code\": example1['code'],\n",
    "                \"example2_prompt\": example2['prompt'],\n",
    "                \"example2_code\": example2['code'],\n",
    "                \"generated_code\": generated_code\n",
    "            }\n",
    "\n",
    "            # Generate the prompt for the current Python file\n",
    "            result = chain.invoke(inputs)\n",
    "            generated_prompt = result['text'].strip()\n",
    "\n",
    "            # Store the generated prompt and the corresponding Python code in the dictionary\n",
    "            generated_prompts[generated_prompt] = generated_code\n",
    "\n",
    "    return generated_prompts\n",
    "\n",
    "# Step 5: Define the folder where all generated Python files are saved\n",
    "generated_code_folder = r'C:\\Users\\bhull\\Desktop\\UCLA Grad\\Spring 2024\\CS 219\\219_final_project\\LLM-assisted_mobile_trace_analysis\\modified_generated_dataset'\n",
    "\n",
    "# Step 6: Generate the prompts for the Python files\n",
    "generated_dataset = generate_prompts_for_code(generated_code_folder, real_prompt_code_pairs)\n",
    "\n",
    "# Step 7: Save the generated dataset (optional, saving as JSON)\n",
    "with open('modified_synthetic_dataset.json', 'w') as f:\n",
    "    json.dump(generated_dataset, f, indent=4)\n",
    "\n",
    "print(\"Modified synthetic dataset created and saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
